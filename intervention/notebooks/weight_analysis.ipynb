{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a6c8ae374b4d378e9736704356de1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example: Load a pre-trained BERT model from Hugging Face Transformers\n",
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight\n",
      "embeddings.token_type_embeddings.weight\n",
      "embeddings.LayerNorm.weight\n",
      "embeddings.LayerNorm.bias\n",
      "encoder.layer.0.attention.self.query.weight\n",
      "encoder.layer.0.attention.self.query.bias\n",
      "encoder.layer.0.attention.self.key.weight\n",
      "encoder.layer.0.attention.self.key.bias\n",
      "encoder.layer.0.attention.self.value.weight\n",
      "encoder.layer.0.attention.self.value.bias\n",
      "encoder.layer.0.attention.output.dense.weight\n",
      "encoder.layer.0.attention.output.dense.bias\n",
      "encoder.layer.0.attention.output.LayerNorm.weight\n",
      "encoder.layer.0.attention.output.LayerNorm.bias\n",
      "encoder.layer.0.intermediate.dense.weight\n",
      "encoder.layer.0.intermediate.dense.bias\n",
      "encoder.layer.0.output.dense.weight\n",
      "encoder.layer.0.output.dense.bias\n",
      "encoder.layer.0.output.LayerNorm.weight\n",
      "encoder.layer.0.output.LayerNorm.bias\n",
      "encoder.layer.1.attention.self.query.weight\n",
      "encoder.layer.1.attention.self.query.bias\n",
      "encoder.layer.1.attention.self.key.weight\n",
      "encoder.layer.1.attention.self.key.bias\n",
      "encoder.layer.1.attention.self.value.weight\n",
      "encoder.layer.1.attention.self.value.bias\n",
      "encoder.layer.1.attention.output.dense.weight\n",
      "encoder.layer.1.attention.output.dense.bias\n",
      "encoder.layer.1.attention.output.LayerNorm.weight\n",
      "encoder.layer.1.attention.output.LayerNorm.bias\n",
      "encoder.layer.1.intermediate.dense.weight\n",
      "encoder.layer.1.intermediate.dense.bias\n",
      "encoder.layer.1.output.dense.weight\n",
      "encoder.layer.1.output.dense.bias\n",
      "encoder.layer.1.output.LayerNorm.weight\n",
      "encoder.layer.1.output.LayerNorm.bias\n",
      "encoder.layer.2.attention.self.query.weight\n",
      "encoder.layer.2.attention.self.query.bias\n",
      "encoder.layer.2.attention.self.key.weight\n",
      "encoder.layer.2.attention.self.key.bias\n",
      "encoder.layer.2.attention.self.value.weight\n",
      "encoder.layer.2.attention.self.value.bias\n",
      "encoder.layer.2.attention.output.dense.weight\n",
      "encoder.layer.2.attention.output.dense.bias\n",
      "encoder.layer.2.attention.output.LayerNorm.weight\n",
      "encoder.layer.2.attention.output.LayerNorm.bias\n",
      "encoder.layer.2.intermediate.dense.weight\n",
      "encoder.layer.2.intermediate.dense.bias\n",
      "encoder.layer.2.output.dense.weight\n",
      "encoder.layer.2.output.dense.bias\n",
      "encoder.layer.2.output.LayerNorm.weight\n",
      "encoder.layer.2.output.LayerNorm.bias\n",
      "encoder.layer.3.attention.self.query.weight\n",
      "encoder.layer.3.attention.self.query.bias\n",
      "encoder.layer.3.attention.self.key.weight\n",
      "encoder.layer.3.attention.self.key.bias\n",
      "encoder.layer.3.attention.self.value.weight\n",
      "encoder.layer.3.attention.self.value.bias\n",
      "encoder.layer.3.attention.output.dense.weight\n",
      "encoder.layer.3.attention.output.dense.bias\n",
      "encoder.layer.3.attention.output.LayerNorm.weight\n",
      "encoder.layer.3.attention.output.LayerNorm.bias\n",
      "encoder.layer.3.intermediate.dense.weight\n",
      "encoder.layer.3.intermediate.dense.bias\n",
      "encoder.layer.3.output.dense.weight\n",
      "encoder.layer.3.output.dense.bias\n",
      "encoder.layer.3.output.LayerNorm.weight\n",
      "encoder.layer.3.output.LayerNorm.bias\n",
      "encoder.layer.4.attention.self.query.weight\n",
      "encoder.layer.4.attention.self.query.bias\n",
      "encoder.layer.4.attention.self.key.weight\n",
      "encoder.layer.4.attention.self.key.bias\n",
      "encoder.layer.4.attention.self.value.weight\n",
      "encoder.layer.4.attention.self.value.bias\n",
      "encoder.layer.4.attention.output.dense.weight\n",
      "encoder.layer.4.attention.output.dense.bias\n",
      "encoder.layer.4.attention.output.LayerNorm.weight\n",
      "encoder.layer.4.attention.output.LayerNorm.bias\n",
      "encoder.layer.4.intermediate.dense.weight\n",
      "encoder.layer.4.intermediate.dense.bias\n",
      "encoder.layer.4.output.dense.weight\n",
      "encoder.layer.4.output.dense.bias\n",
      "encoder.layer.4.output.LayerNorm.weight\n",
      "encoder.layer.4.output.LayerNorm.bias\n",
      "encoder.layer.5.attention.self.query.weight\n",
      "encoder.layer.5.attention.self.query.bias\n",
      "encoder.layer.5.attention.self.key.weight\n",
      "encoder.layer.5.attention.self.key.bias\n",
      "encoder.layer.5.attention.self.value.weight\n",
      "encoder.layer.5.attention.self.value.bias\n",
      "encoder.layer.5.attention.output.dense.weight\n",
      "encoder.layer.5.attention.output.dense.bias\n",
      "encoder.layer.5.attention.output.LayerNorm.weight\n",
      "encoder.layer.5.attention.output.LayerNorm.bias\n",
      "encoder.layer.5.intermediate.dense.weight\n",
      "encoder.layer.5.intermediate.dense.bias\n",
      "encoder.layer.5.output.dense.weight\n",
      "encoder.layer.5.output.dense.bias\n",
      "encoder.layer.5.output.LayerNorm.weight\n",
      "encoder.layer.5.output.LayerNorm.bias\n",
      "encoder.layer.6.attention.self.query.weight\n",
      "encoder.layer.6.attention.self.query.bias\n",
      "encoder.layer.6.attention.self.key.weight\n",
      "encoder.layer.6.attention.self.key.bias\n",
      "encoder.layer.6.attention.self.value.weight\n",
      "encoder.layer.6.attention.self.value.bias\n",
      "encoder.layer.6.attention.output.dense.weight\n",
      "encoder.layer.6.attention.output.dense.bias\n",
      "encoder.layer.6.attention.output.LayerNorm.weight\n",
      "encoder.layer.6.attention.output.LayerNorm.bias\n",
      "encoder.layer.6.intermediate.dense.weight\n",
      "encoder.layer.6.intermediate.dense.bias\n",
      "encoder.layer.6.output.dense.weight\n",
      "encoder.layer.6.output.dense.bias\n",
      "encoder.layer.6.output.LayerNorm.weight\n",
      "encoder.layer.6.output.LayerNorm.bias\n",
      "encoder.layer.7.attention.self.query.weight\n",
      "encoder.layer.7.attention.self.query.bias\n",
      "encoder.layer.7.attention.self.key.weight\n",
      "encoder.layer.7.attention.self.key.bias\n",
      "encoder.layer.7.attention.self.value.weight\n",
      "encoder.layer.7.attention.self.value.bias\n",
      "encoder.layer.7.attention.output.dense.weight\n",
      "encoder.layer.7.attention.output.dense.bias\n",
      "encoder.layer.7.attention.output.LayerNorm.weight\n",
      "encoder.layer.7.attention.output.LayerNorm.bias\n",
      "encoder.layer.7.intermediate.dense.weight\n",
      "encoder.layer.7.intermediate.dense.bias\n",
      "encoder.layer.7.output.dense.weight\n",
      "encoder.layer.7.output.dense.bias\n",
      "encoder.layer.7.output.LayerNorm.weight\n",
      "encoder.layer.7.output.LayerNorm.bias\n",
      "encoder.layer.8.attention.self.query.weight\n",
      "encoder.layer.8.attention.self.query.bias\n",
      "encoder.layer.8.attention.self.key.weight\n",
      "encoder.layer.8.attention.self.key.bias\n",
      "encoder.layer.8.attention.self.value.weight\n",
      "encoder.layer.8.attention.self.value.bias\n",
      "encoder.layer.8.attention.output.dense.weight\n",
      "encoder.layer.8.attention.output.dense.bias\n",
      "encoder.layer.8.attention.output.LayerNorm.weight\n",
      "encoder.layer.8.attention.output.LayerNorm.bias\n",
      "encoder.layer.8.intermediate.dense.weight\n",
      "encoder.layer.8.intermediate.dense.bias\n",
      "encoder.layer.8.output.dense.weight\n",
      "encoder.layer.8.output.dense.bias\n",
      "encoder.layer.8.output.LayerNorm.weight\n",
      "encoder.layer.8.output.LayerNorm.bias\n",
      "encoder.layer.9.attention.self.query.weight\n",
      "encoder.layer.9.attention.self.query.bias\n",
      "encoder.layer.9.attention.self.key.weight\n",
      "encoder.layer.9.attention.self.key.bias\n",
      "encoder.layer.9.attention.self.value.weight\n",
      "encoder.layer.9.attention.self.value.bias\n",
      "encoder.layer.9.attention.output.dense.weight\n",
      "encoder.layer.9.attention.output.dense.bias\n",
      "encoder.layer.9.attention.output.LayerNorm.weight\n",
      "encoder.layer.9.attention.output.LayerNorm.bias\n",
      "encoder.layer.9.intermediate.dense.weight\n",
      "encoder.layer.9.intermediate.dense.bias\n",
      "encoder.layer.9.output.dense.weight\n",
      "encoder.layer.9.output.dense.bias\n",
      "encoder.layer.9.output.LayerNorm.weight\n",
      "encoder.layer.9.output.LayerNorm.bias\n",
      "encoder.layer.10.attention.self.query.weight\n",
      "encoder.layer.10.attention.self.query.bias\n",
      "encoder.layer.10.attention.self.key.weight\n",
      "encoder.layer.10.attention.self.key.bias\n",
      "encoder.layer.10.attention.self.value.weight\n",
      "encoder.layer.10.attention.self.value.bias\n",
      "encoder.layer.10.attention.output.dense.weight\n",
      "encoder.layer.10.attention.output.dense.bias\n",
      "encoder.layer.10.attention.output.LayerNorm.weight\n",
      "encoder.layer.10.attention.output.LayerNorm.bias\n",
      "encoder.layer.10.intermediate.dense.weight\n",
      "encoder.layer.10.intermediate.dense.bias\n",
      "encoder.layer.10.output.dense.weight\n",
      "encoder.layer.10.output.dense.bias\n",
      "encoder.layer.10.output.LayerNorm.weight\n",
      "encoder.layer.10.output.LayerNorm.bias\n",
      "encoder.layer.11.attention.self.query.weight\n",
      "encoder.layer.11.attention.self.query.bias\n",
      "encoder.layer.11.attention.self.key.weight\n",
      "encoder.layer.11.attention.self.key.bias\n",
      "encoder.layer.11.attention.self.value.weight\n",
      "encoder.layer.11.attention.self.value.bias\n",
      "encoder.layer.11.attention.output.dense.weight\n",
      "encoder.layer.11.attention.output.dense.bias\n",
      "encoder.layer.11.attention.output.LayerNorm.weight\n",
      "encoder.layer.11.attention.output.LayerNorm.bias\n",
      "encoder.layer.11.intermediate.dense.weight\n",
      "encoder.layer.11.intermediate.dense.bias\n",
      "encoder.layer.11.output.dense.weight\n",
      "encoder.layer.11.output.dense.bias\n",
      "encoder.layer.11.output.LayerNorm.weight\n",
      "encoder.layer.11.output.LayerNorm.bias\n",
      "pooler.dense.weight\n",
      "pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    #if 'weight' in name:  # Filter out only the weights\n",
    "        # plt.figure(figsize=(10, 5))\n",
    "        # plt.title(f\"Distribution of Weights for {name}\")\n",
    "        # plt.hist(param.detach().cpu().numpy().flatten(), bins=100, density=True)\n",
    "        # plt.xlabel('Weight Value')\n",
    "        # plt.ylabel('Density')\n",
    "        # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.attention.self.query.weight'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = \"encoder.layer.0.attention.self.query.weight\"\n",
    "name.split(\"layer.\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention.self.query.weight\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "name = \"encoder.layer.0.attention.self.query.weight\"\n",
    "processed_layer = name.split(\"layer.\")[1]\n",
    "layer_name = processed_layer[2:]\n",
    "layer_num = processed_layer[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight\n",
      "embeddings.token_type_embeddings.weight\n",
      "embeddings.LayerNorm.weight\n",
      "pooler.dense.weight\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of parameter names\n",
    "param_names = [\n",
    "    \"encoder.layer.0.attention.self.query.weight\", \"encoder.layer.0.attention.self.query.bias\",\n",
    "    \"encoder.layer.0.attention.self.key.weight\", \"encoder.layer.0.attention.self.key.bias\",\n",
    "    \"encoder.layer.0.attention.self.value.weight\", \"encoder.layer.0.attention.self.value.bias\",\n",
    "    \"encoder.layer.0.attention.output.dense.weight\", \"encoder.layer.0.attention.output.dense.bias\",\n",
    "    \"encoder.layer.0.attention.output.LayerNorm.weight\", \"encoder.layer.0.attention.output.LayerNorm.bias\",\n",
    "    \"encoder.layer.0.intermediate.dense.weight\", \"encoder.layer.0.intermediate.dense.bias\",\n",
    "    \"encoder.layer.0.output.dense.weight\", \"encoder.layer.0.output.dense.bias\",\n",
    "    \"encoder.layer.0.output.LayerNorm.weight\", \"encoder.layer.0.output.LayerNorm.bias\",\n",
    "    \"encoder.layer.1.attention.self.query.weight\", \"encoder.layer.1.attention.self.query.bias\",\n",
    "    \"encoder.layer.1.attention.self.key.weight\", \"encoder.layer.1.attention.self.key.bias\",\n",
    "    \"encoder.layer.1.attention.self.value.weight\", \"encoder.layer.1.attention.self.value.bias\",\n",
    "    \"encoder.layer.1.attention.output.dense.weight\", \"encoder.layer.1.attention.output.dense.bias\",\n",
    "    \"encoder.layer.1.attention.output.LayerNorm.weight\", \"encoder.layer.1.attention.output.LayerNorm.bias\",\n",
    "    \"encoder.layer.1.intermediate.dense.weight\", \"encoder.layer.1.intermediate.dense.bias\"\n",
    "]\n",
    "\n",
    "# Dictionary to store the grouped parameters\n",
    "grouped_params = {}\n",
    "\n",
    "# Group parameters by layer number and type\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        try:\n",
    "            processed_layer = name.split(\"layer.\")[1]\n",
    "            param_type = processed_layer[2:]\n",
    "            layer_num = processed_layer[0]\n",
    "                \n",
    "            if param_type not in grouped_params:\n",
    "                grouped_params[param_type] = {}\n",
    "            grouped_params[param_type][layer_num] = param\n",
    "        \n",
    "        except: \n",
    "            print(name)\n",
    "\n",
    "# # Plot the distribution of attention query weights for each layer\n",
    "# for layer_num, layer_params in grouped_params.items():\n",
    "#     query_weight = layer_params[\"attention.self.query.weight\"]\n",
    "#     # Retrieve the actual weight values for the query weight parameter\n",
    "#     # and plot the distribution using matplotlib\n",
    "#     # For example:\n",
    "#     # plt.hist(query_weight_values, alpha=0.5, label=f\"Layer {layer_num}\")\n",
    "\n",
    "# # Add legend and labels to the plot\n",
    "# plt.legend()\n",
    "# plt.xlabel(\"Weight Values\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.title(\"Distribution of Attention Query Weights\")\n",
    "\n",
    "# # Display the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['attention.self.query.weight', 'attention.self.key.weight', 'attention.self.value.weight', 'attention.output.dense.weight', 'attention.output.LayerNorm.weight', 'intermediate.dense.weight', 'output.dense.weight', 'output.LayerNorm.weight', '.attention.self.query.weight', '.attention.self.key.weight', '.attention.self.value.weight', '.attention.output.dense.weight', '.attention.output.LayerNorm.weight', '.intermediate.dense.weight', '.output.dense.weight', '.output.LayerNorm.weight'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': Parameter containing:\n",
       " tensor([[-0.0164,  0.0261, -0.0263,  ...,  0.0154,  0.0768,  0.0548],\n",
       "         [-0.0326,  0.0346, -0.0423,  ..., -0.0527,  0.1393,  0.0078],\n",
       "         [ 0.0105,  0.0334,  0.0109,  ..., -0.0279,  0.0258, -0.0468],\n",
       "         ...,\n",
       "         [-0.0085,  0.0514,  0.0555,  ...,  0.0282,  0.0543, -0.0541],\n",
       "         [-0.0198,  0.0944,  0.0617,  ..., -0.1042,  0.0601,  0.0470],\n",
       "         [ 0.0015, -0.0952,  0.0099,  ..., -0.0191, -0.0508, -0.0085]],\n",
       "        requires_grad=True),\n",
       " '1': Parameter containing:\n",
       " tensor([[ 0.0313, -0.0256,  0.0050,  ..., -0.0338, -0.0593,  0.0650],\n",
       "         [-0.0134, -0.0245, -0.0281,  ..., -0.0349,  0.0525,  0.0496],\n",
       "         [-0.0776, -0.0500, -0.0082,  ...,  0.0184,  0.0116, -0.0470],\n",
       "         ...,\n",
       "         [ 0.0597, -0.0208, -0.0365,  ...,  0.0078,  0.0104, -0.0282],\n",
       "         [ 0.0063, -0.0108, -0.0043,  ..., -0.0039,  0.0234, -0.0588],\n",
       "         [-0.0128, -0.0117,  0.0549,  ...,  0.1037, -0.0565,  0.0244]],\n",
       "        requires_grad=True),\n",
       " '2': Parameter containing:\n",
       " tensor([[ 0.0843,  0.0147, -0.0630,  ...,  0.0154, -0.0918, -0.0197],\n",
       "         [ 0.0958,  0.0032, -0.0573,  ...,  0.0731, -0.0140, -0.0302],\n",
       "         [ 0.0754,  0.0281, -0.0567,  ...,  0.0433, -0.0351,  0.0511],\n",
       "         ...,\n",
       "         [ 0.0245,  0.0879, -0.0444,  ...,  0.0082, -0.0464, -0.0239],\n",
       "         [-0.0025, -0.0260,  0.0062,  ..., -0.0296, -0.0272, -0.0281],\n",
       "         [-0.0127, -0.0188,  0.0386,  ..., -0.0165, -0.0435,  0.0492]],\n",
       "        requires_grad=True),\n",
       " '3': Parameter containing:\n",
       " tensor([[ 0.0586, -0.0279,  0.0022,  ..., -0.0205,  0.0419,  0.0560],\n",
       "         [-0.0011,  0.1283, -0.0564,  ..., -0.0514, -0.0199,  0.0516],\n",
       "         [-0.0083,  0.0060, -0.0469,  ...,  0.0569, -0.0073, -0.0616],\n",
       "         ...,\n",
       "         [ 0.0118,  0.0645, -0.0159,  ..., -0.0027, -0.0031, -0.0141],\n",
       "         [ 0.0029,  0.0531,  0.0221,  ..., -0.0047,  0.0399,  0.0344],\n",
       "         [ 0.0512,  0.0246,  0.0077,  ...,  0.0108,  0.0262,  0.0081]],\n",
       "        requires_grad=True),\n",
       " '4': Parameter containing:\n",
       " tensor([[ 0.0830,  0.0501,  0.0218,  ..., -0.0395,  0.0418,  0.0401],\n",
       "         [ 0.0208, -0.0456, -0.0103,  ...,  0.0400,  0.0352,  0.0081],\n",
       "         [ 0.0653,  0.0392,  0.0233,  ..., -0.0624,  0.0301,  0.0036],\n",
       "         ...,\n",
       "         [-0.0159, -0.0142, -0.0249,  ...,  0.0306,  0.0291,  0.0581],\n",
       "         [-0.0365, -0.0181, -0.0304,  ..., -0.0623,  0.0352, -0.0063],\n",
       "         [-0.0340,  0.0211,  0.0738,  ..., -0.0090, -0.0281, -0.0197]],\n",
       "        requires_grad=True),\n",
       " '5': Parameter containing:\n",
       " tensor([[-0.0086,  0.0009,  0.0338,  ...,  0.0474, -0.0108,  0.0122],\n",
       "         [-0.0392,  0.0081, -0.0889,  ...,  0.0764, -0.0025,  0.0353],\n",
       "         [ 0.0255,  0.0334,  0.0309,  ...,  0.0515,  0.0002,  0.0151],\n",
       "         ...,\n",
       "         [-0.0119,  0.0519,  0.0147,  ..., -0.0491, -0.0295,  0.0240],\n",
       "         [ 0.0075,  0.0113, -0.0348,  ..., -0.0418,  0.0076,  0.0506],\n",
       "         [ 0.0414, -0.0987,  0.0465,  ...,  0.0614, -0.0103,  0.0963]],\n",
       "        requires_grad=True),\n",
       " '6': Parameter containing:\n",
       " tensor([[-0.0253,  0.0489, -0.0148,  ...,  0.0431, -0.0253,  0.0013],\n",
       "         [-0.0143, -0.0465, -0.0020,  ..., -0.0073, -0.0296,  0.0208],\n",
       "         [ 0.0147, -0.0400, -0.0344,  ..., -0.0697, -0.1240, -0.0031],\n",
       "         ...,\n",
       "         [-0.0214, -0.0160, -0.0680,  ...,  0.0233, -0.0359,  0.0355],\n",
       "         [-0.0297,  0.0270,  0.0464,  ...,  0.0691,  0.0293,  0.0095],\n",
       "         [-0.0012, -0.0403,  0.0343,  ..., -0.0742,  0.0371,  0.0421]],\n",
       "        requires_grad=True),\n",
       " '7': Parameter containing:\n",
       " tensor([[-0.0008, -0.0777, -0.0345,  ..., -0.0127, -0.0085,  0.0286],\n",
       "         [-0.0150, -0.0272, -0.0251,  ..., -0.0537, -0.0197, -0.0694],\n",
       "         [ 0.0409, -0.0108, -0.0435,  ..., -0.0075,  0.0340, -0.0098],\n",
       "         ...,\n",
       "         [-0.0181,  0.0073,  0.0136,  ..., -0.0587, -0.0276,  0.0417],\n",
       "         [-0.0015,  0.0465, -0.0191,  ...,  0.0133,  0.0922,  0.0024],\n",
       "         [-0.0635, -0.0008,  0.0355,  ...,  0.0524,  0.0080, -0.0208]],\n",
       "        requires_grad=True),\n",
       " '8': Parameter containing:\n",
       " tensor([[ 0.0449,  0.0627,  0.0169,  ..., -0.0452, -0.0251,  0.0063],\n",
       "         [ 0.0229, -0.0180,  0.0190,  ...,  0.0315, -0.0022,  0.0481],\n",
       "         [ 0.0308, -0.1261,  0.0070,  ..., -0.0444,  0.0413, -0.0345],\n",
       "         ...,\n",
       "         [-0.0029, -0.0355, -0.0323,  ..., -0.0218, -0.0045, -0.0325],\n",
       "         [ 0.0047, -0.0320, -0.0533,  ...,  0.0862, -0.0552,  0.0023],\n",
       "         [-0.0213,  0.0550,  0.0541,  ...,  0.0029,  0.0172,  0.0268]],\n",
       "        requires_grad=True),\n",
       " '9': Parameter containing:\n",
       " tensor([[ 0.0800,  0.0057, -0.0042,  ..., -0.0356, -0.0040,  0.0070],\n",
       "         [-0.0144, -0.0531, -0.0611,  ..., -0.0048, -0.0642,  0.0032],\n",
       "         [-0.0422,  0.0086, -0.0152,  ..., -0.0499,  0.0145,  0.0231],\n",
       "         ...,\n",
       "         [-0.0032, -0.0186,  0.0253,  ..., -0.0898, -0.0989,  0.0352],\n",
       "         [ 0.0277,  0.0576, -0.0266,  ...,  0.0038,  0.0111, -0.0412],\n",
       "         [-0.0708,  0.0044,  0.0179,  ..., -0.0138, -0.0296, -0.0328]],\n",
       "        requires_grad=True)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_params['attention.self.query.weight']#.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertModel' object has no attribute 'trainable_variables'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m weight_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Collect weights\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mname:  \u001b[38;5;66;03m# Focus on kernel weights, equivalent to 'weight' in PyTorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         layer_name \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m layer\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Simplify layer names\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_updated/lib/python3.9/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertModel' object has no attribute 'trainable_variables'"
     ]
    }
   ],
   "source": [
    "# Dictionary to collect weights\n",
    "weight_dict = {}\n",
    "\n",
    "# Collect weights\n",
    "for layer in model.trainable_variables:\n",
    "    if 'kernel' in layer.name:  # Focus on kernel weights, equivalent to 'weight' in PyTorch\n",
    "        layer_name = layer.name.split('/')[-2] + '.' + layer.name.split('/')[-1]  # Simplify layer names\n",
    "        if layer_name not in weight_dict:\n",
    "            weight_dict[layer_name] = []\n",
    "        weight_dict[layer_name].append(layer.numpy().flatten())\n",
    "\n",
    "# Plot weights\n",
    "for key, weights in weight_dict.items():\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(f\"Distribution of {key} Weights Across Layers\")\n",
    "    for layer_weights in weights:\n",
    "        plt.hist(layer_weights, bins=100, density=True, alpha=0.5, label=f'Layer {weights.index(layer_weights)+1}')\n",
    "    plt.xlabel('Weight Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_updated",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
