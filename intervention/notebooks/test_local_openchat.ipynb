{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c719b29157403cb87b2099050cf53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import llama\n",
    "import torch \n",
    "MODEL = \"openchat/openchat_3.5\"\n",
    "\n",
    "LOAD_8BIT = False # True\n",
    "\n",
    "tokenizer = llama.LlamaTokenizer.from_pretrained(MODEL)\n",
    "model = llama.LlamaForCausalLM.from_pretrained(MODEL, low_cpu_mem_usage=True, load_in_8bit=LOAD_8BIT, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate.utils import BnbQuantizationConfig\n",
    "\n",
    "bnb_quantization_config = BnbQuantizationConfig(load_in_8bit=True, llm_int8_threshold = 6)\n",
    "\n",
    "from accelerate.utils import load_and_quantize_model\n",
    "quantized_model = load_and_quantize_model(empty_model, weights_location=weights_location, bnb_quantization_config=bnb_quantization_config, device_map = \"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = tokenizer([\"hi\"], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 12014]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i['input_ids'].to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = model.model.embed_tokens(i['input_ids'].to(\"cuda\"))#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaAttention(\n",
       "  (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "  (k_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "  (v_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
       "  (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "  (rotary_emb): LlamaRotaryEmbedding()\n",
       "  (att_out): Identity()\n",
       "  (value_out): Identity()\n",
       "  (head_out): Identity()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.8179e-05, -2.2531e-05, -4.4465e-05,  ...,  7.2598e-05,\n",
       "            2.6822e-06,  1.3590e-05],\n",
       "          [ 1.8179e-05, -2.2531e-05, -4.4465e-05,  ...,  7.2598e-05,\n",
       "            2.6822e-06,  1.3590e-05]]], device='cuda:0', dtype=torch.float16,\n",
       "        grad_fn=<MatMul8bitLtBackward>),\n",
       " None,\n",
       " None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Identity()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.head_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Identity()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_module(model, name):\n",
    "    \"\"\"\n",
    "    Finds the named module within the given model.\n",
    "    \"\"\"\n",
    "    for n, m in model.named_modules():\n",
    "        if n == name:\n",
    "            return m\n",
    "    raise LookupError(name)\n",
    "\n",
    "\n",
    "module = model\n",
    "layer = 'model.layers.31.self_attn.head_out'\n",
    "get_module(module, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
