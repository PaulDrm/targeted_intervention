{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Notebook to run requirement verfication for capella models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot load PVMT extension: ValueError: Provided model does not have a PropertyValuePkg\n",
      "Property values are not available in this model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<capellambse.model.MelodyModel at 0x7fb5f4607ca0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import capellambse\n",
    "path_to_model = \"./capella/moonbase/SGAC_Moon_Base/IP_New.aird\"\n",
    "model_capella = capellambse.MelodyModel(path_to_model)\n",
    "model_capella"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get triples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/miniconda3/envs/llm_updated/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================\n",
      "Input query: The settlement shall enable regular crew and cargo access to the lunar surface (and vice versa) via airlock or pressurized rover.\n",
      "\n",
      "Top most similar entries in Knowledge Graph:\n",
      "Settlement Peripherals & Payloads (Score: 0.5564)\n",
      "Habitation (Score: 0.4687)\n",
      "Pressurized Rover (Score: 0.3940)\n",
      "Airlock (Score: 0.2821)\n",
      "Lander (Score: 0.2003)\n",
      "Surface Robotics (Score: 0.1974)\n",
      "Uncrewed Surface Robotics (Score: 0.1813)\n",
      "Structure, Shielding, & Storage (Score: 0.1680)\n",
      "Thermal Control (Score: 0.1589)\n",
      "Waste Management (Score: 0.1508)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch \n",
    "#req = \"The settlement shall enable regular crew and cargo access to the lunar surface (and vice versa) via airlock or pressurized rover.\"\n",
    "#req = \"Airlock\"\n",
    "#req = \"The Space Shuttle shall provide the basic transportation for all phases of the HST program including deployment, on-orbit servicing, and reboost or return to earth.\"\n",
    "#req = \"A scientific measurement capability is provided through a complement of up to four axial scientific instruments and a Fine Guidance Sensor.\"\n",
    "#req = \"All normal forward and return link data transmission shall be via the NASA Communications Network (NASCOM) and the Space Network (SN).\"\n",
    "#req = \"The spacecraft shall have a mass < 11500kg.\"\n",
    "#req = \"One instrument shall be capable of detecting planets located more than 800 million kilometers (800 x 10^6 km) from their host star, in star systems situated at least 10 light-years away from the Solar System. Note: 1 light-year is approximately equal to 10 trillion kilometers (10 x 10^13 km).\"\n",
    "req = \"Observational capability shall be provided by the instruments for the following: a. Visible photometric imaging b. Ultraviolet spectrophotometry c. Near infrared spectrophotometry (> 1 micron) and imaging\"\n",
    "req = \"The settlement shall enable regular crew and cargo access to the lunar surface (and vice versa) via airlock or pressurized rover.\"\n",
    "#req = \"Crews shall be able to safely inhabit the settlement for up to 12 mo. at a time. NASA lunar orbital + surface reference mission (Antonsen and Taddeo, 2020).\"\n",
    "query = req\n",
    "\n",
    "names = [comp.name for comp in model_capella.la.all_components]\n",
    "\n",
    "model_emb = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "query_embedding = model_emb.encode(req, convert_to_tensor=True)\n",
    "\n",
    "embeds = model_emb.encode(names, convert_to_tensor=True)\n",
    "\n",
    "top_k = 10\n",
    "\n",
    "cos_scores = util.cos_sim(query_embedding, embeds)[0]\n",
    "top_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "\n",
    "#print(\"\\n\\n======================\\n\\n\")\n",
    "print(\"======================\")\n",
    "print(\"Input query:\", query)\n",
    "print(f\"\\nTop most similar entries in Knowledge Graph:\")\n",
    "\n",
    "#iids = [iids[idx] for idx in top_results.indices.tolicst()]\n",
    "#names= get_names([iids[idx] for idx in top_results.indices.tolist()], session) # inputs['artifact']['type'],\n",
    "# scores = [top_results[idx] for idx in top_results.indices.tolist()]\n",
    "# print(names)\n",
    "for score, idx in zip(top_results[0], top_results[1]):\n",
    "#for score, name in zip(top_results[0], names):\n",
    "    # print(iids[idx], \"(Score: {:.4f})\".format(score))\n",
    "    print(names[idx], \"(Score: {:.4f})\".format(score))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reranking with Cross-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  9.4506],\n",
      "        [-11.1605]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"cross-encoder/ms-marco-MiniLM-L-12-v2\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "features = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = model(**features).logits\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "model_name = \"cross-encoder/ms-marco-MiniLM-L-12-v2\"\n",
    "cross_encoder = CrossEncoder(model_name, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "\n",
      "Top-3 Cross-Encoder Re-ranker hits\n",
      "\t-3.290\tSettlement Peripherals & Payloads\n",
      "\t-5.324\tPressurized Rover\n",
      "\t-5.556\tAirlock\n"
     ]
    }
   ],
   "source": [
    "##### Re-Ranking #####\n",
    "# Now, score all retrieved passages with the cross_encoder\n",
    "query = req\n",
    "entities = [names[idx] for idx in top_results.indices.tolist()]\n",
    "cross_inp = [[query, entity] for entity in entities]\n",
    "cross_scores = cross_encoder.predict(cross_inp)\n",
    "hits = [{'entity': ent} for ent in entities]\n",
    "# Sort results by the cross-encoder scores\n",
    "for idx in range(len(cross_scores)):\n",
    "    hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "# Output of top-5 hits from re-ranker\n",
    "print(\"\\n-------------------------\\n\")\n",
    "print(\"Top-3 Cross-Encoder Re-ranker hits\")\n",
    "hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "for hit in hits[0:3]:\n",
    "    print(\"\\t{:.3f}\\t{}\".format(hit['cross-score'], hit[\"entity\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reranking with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.99s/it]\n",
      "/home/paul/miniconda3/envs/llm_updated/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM#, # AutoModelForCausalLM, AutoTokenizer#, LlamaForCausalLM, LlamaTokenizer #, OPTForCausalLM, AutoModelForCausalLM\n",
    "import torch \n",
    "MODEL_NAME =\"openchat/openchat_3.5\"#\"lmsys/vicuna-13b-v1.5\"#\"openchat/openchat_3.5\"#\"lmsys/vicuna-13b-v1.5\" #\"mistralai/Mistral-7B-v0.1\"\n",
    "LOAD_8BIT = True \n",
    "# #LlamaForCausalLM.from_pretrained(MODEL_NAME,\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME,\n",
    "        load_in_8bit=LOAD_8BIT,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        )\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if \"open\" in model.config._name_or_path:\n",
    "\n",
    "    model.generation_config.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "def evaluate(\n",
    "    prompt,\n",
    "    model = None,\n",
    "    tokenizer = None,\n",
    "    stopping_criteria = None,\n",
    "    device = 'cuda',\n",
    "    **kwargs,\n",
    "):\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    generation_config = GenerationConfig(\n",
    "        **kwargs,\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            output_hidden_states= True,\n",
    "            stopping_criteria= stopping_criteria\n",
    "        )\n",
    "        s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    return output, generation_output\n",
    "\n",
    "MAX_NEW_TOKENS = 600\n",
    "extra_args = {\"max_new_tokens\":MAX_NEW_TOKENS,\n",
    "          #  \"min_new_tokens\": 256, \n",
    "             \"do_sample\":False,#True, \n",
    "            \"num_beams\" : 1, \n",
    "            \"num_return_sequences\" : 1, \n",
    "            #\"no_repeat_ngram_size\": 12, \n",
    "            \"temperature\": 0.5, \n",
    "            \"top_p\": 0.95,\n",
    "          #  \"begin_suppress_tokens\": [2], \n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/miniconda3/envs/llm_updated/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/paul/miniconda3/envs/llm_updated/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/paul/miniconda3/envs/llm_updated/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "entities = [names[idx] for idx in top_results.indices.tolist()]\n",
    "requirement = req\n",
    "prompt = f\"\"\"GPT4 Correct User: Based on the requirement what is the most relevant entities? Return only a Python list object.\n",
    "Entities are: {entities}\n",
    "Requirement: {requirement}\n",
    "<|end of turn|>GPT4 Correct Assistant: Entity:\n",
    "\"\"\"\n",
    "extra_args = {\"max_new_tokens\":MAX_NEW_TOKENS,\n",
    "          #  \"min_new_tokens\": 256, \n",
    "             \"do_sample\":False,#True, \n",
    "            \"num_beams\" : 1, \n",
    "            \"num_return_sequences\" : 1, \n",
    "            #\"no_repeat_ngram_size\": 12, \n",
    "            \"temperature\": 0.5, \n",
    "            \"top_p\": 0.95,\n",
    "          #  \"begin_suppress_tokens\": [2], \n",
    "            }\n",
    "output, complete_output = evaluate(prompt, model = model, tokenizer= tokenizer,  device = \"cuda\", **extra_args) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT4 Correct User: Based on the requirement what is the most relevant entities? Return only a Python list object.\n",
      "Entities are: ['Settlement Peripherals & Payloads', 'Habitation', 'Pressurized Rover', 'Airlock', 'Lander', 'Surface Robotics', 'Uncrewed Surface Robotics', 'Structure, Shielding, & Storage', 'Thermal Control', 'Waste Management']\n",
      "Requirement: The settlement shall enable regular crew and cargo access to the lunar surface (and vice versa) via airlock or pressurized rover.\n",
      "<|end of turn|>GPT4 Correct Assistant: Entity:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> GPT4 Correct User: Based on the requirement what is the most relevant entities? Return only a Python list object.\n",
      "Entities are: ['Settlement Peripherals & Payloads', 'Habitation', 'Pressurized Rover', 'Airlock', 'Lander', 'Surface Robotics', 'Uncrewed Surface Robotics', 'Structure, Shielding, & Storage', 'Thermal Control', 'Waste Management']\n",
      "Requirement: The settlement shall enable regular crew and cargo access to the lunar surface (and vice versa) via airlock or pressurized rover.\n",
      "<|end of turn|>GPT4 Correct Assistant: Entity:\n",
      "['Airlock', 'Pressurized Rover']\n",
      "<|end of turn|><|end_of_turn|>\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/miniconda3/envs/llm_updated/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/paul/miniconda3/envs/llm_updated/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/paul/miniconda3/envs/llm_updated/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/paul/miniconda3/envs/llm_updated/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "def parse_output(output, prompt_reason, tokenizer):\n",
    "\n",
    "    output =output[len(tokenizer.decode(tokenizer.encode(prompt_reason))):]\n",
    "    return output\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.split(\"<|end of turn|>\")[0]\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "parsed_output = parse_output(output, prompt, tokenizer)\n",
    "parsed_output = clean_text(parsed_output)\n",
    "parsed_output\n",
    "\n",
    "\n",
    "def rerank(model, tokenizer, prompt, extra_args): \n",
    "\n",
    "    output, complete_output = evaluate(prompt, model = model, tokenizer= tokenizer,  device = \"cuda\", **extra_args)\n",
    "    parsed_output = parse_output(output, prompt, tokenizer)\n",
    "    parsed_output = clean_text(parsed_output)\n",
    "    parsed_output = literal_eval(parsed_output)\n",
    "\n",
    "    assert type(parsed_output) == list\n",
    "    return parsed_output\n",
    "\n",
    "entities = [names[idx] for idx in top_results.indices.tolist()]\n",
    "requirement = req\n",
    "prompt = f\"\"\"GPT4 Correct User: Based on the requirement what is the most relevant entities? Return only a Python list object.\n",
    "Entities are: {entities}\n",
    "Requirement: {requirement}\n",
    "<|end of turn|>GPT4 Correct Assistant: Entity:\n",
    "\"\"\"\n",
    "\n",
    "entities = rerank(model, tokenizer, prompt, extra_args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = \"Habitation\"\n",
    "entity = model_capella.la.all_components.by_name(entity)\n",
    "\n",
    "entity = \"Exercise Facilities\"\n",
    "#model_capella.la.all_components.by_name(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_capella.la.all_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2f60ca8e-200e-420a-977a-5dda7c420641', '3585479a-6de3-49b0-ab88-787e38c40d36', '9e7257e0-56d7-4195-a9d0-79fbe540dd5c', '5c6a0f7b-5fd6-4b1c-8a58-b9693ba911cb'}\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "def get_neighbors_component_bfs(root_component, max_triples=None):\n",
    "    queue = deque([root_component])\n",
    "    triples = []\n",
    "    visited = set()\n",
    "\n",
    "    while queue:\n",
    "        \n",
    "        if max_triples and len(triples) >= max_triples:\n",
    "            break  # Stop if the maximum number of triples has been reached\n",
    "        #print(len(triples))\n",
    "        component = queue.popleft()\n",
    "        #print(component.name)\n",
    "        ini_id = component.uuid\n",
    "\n",
    "        if ini_id in visited:\n",
    "            continue\n",
    "\n",
    "        visited.add(ini_id)\n",
    "\n",
    "        # Process direct component neighbors\n",
    "        for comp in component.components:\n",
    "            triples.append((comp.xtype, ini_id, comp.uuid, component.name, \"directly_contains\", comp.name))\n",
    "            queue.append(comp)\n",
    "        \n",
    "        # Add parent to the queue\n",
    "        try:\n",
    "            triples.append((component.xtype, component.owner.uuid, ini_id, component.name, \"is_contained_by\", component.owner.name))\n",
    "            queue.append(component.owner)\n",
    "        except AttributeError: \n",
    "            print(component.name, \"has no owner\")\n",
    "\n",
    "        # Add constraints\n",
    "        for const in component.constraints:\n",
    "            triples.append((const.xtype, ini_id, const.uuid, component.name, \"constraint\", const.name))\n",
    "\n",
    "        # Add property values\n",
    "        for prop in component.property_values:\n",
    "            triples.append((prop.xtype, ini_id, prop.uuid, component.name, \"has\", prop.name +\" \" + str(prop.value)))\n",
    "\n",
    "        try: \n",
    "            # Add component exchanges\n",
    "            for exch in component.related_exchanges:\n",
    "                if exch.target.owner.name != component.name: \n",
    "                    target = exch.target\n",
    "                else: \n",
    "                    target = exch.source\n",
    "                triples.append((target.owner.xtype, ini_id, target.owner.uuid, component.name, exch.name, target.owner.name))\n",
    "                queue.append(target.owner)\n",
    "                \n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        try:    \n",
    "            # Process functions and related exchanges\n",
    "            for func in component.allocated_functions:\n",
    "                triples.append((func.xtype, ini_id, func.uuid, component.name, \"directly_performs\", func.name))\n",
    "                process_function_exchanges(func, triples, component, queue, visited)\n",
    "\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        #print(triples)\n",
    "    \n",
    "    #print(len(triples))\n",
    "    print(visited)\n",
    "    return triples\n",
    "\n",
    "def process_function_exchanges(func, triples, component, queue, visited):\n",
    "    # Process inputs and outputs of the function\n",
    "    for input in func.inputs:\n",
    "        if len(input.exchanges) == 1:\n",
    "            source = input.exchanges[0]\n",
    "            triples.append((source.xtype, func.uuid, source.source.owner.uuid, func.name, source.name, source.source.owner.name))\n",
    "            add_parent_triple(source.source.owner, triples, queue, visited)\n",
    "\n",
    "    for output in func.outputs:\n",
    "        if len(output.exchanges) == 1:\n",
    "            target = output.exchanges[0]\n",
    "            triples.append((target.xtype, func.uuid, target.target.owner.uuid, func.name, target.name, target.target.owner.name))\n",
    "            add_parent_triple(target.target.owner, triples, queue, visited)\n",
    "\n",
    "def add_parent_triple(parent, triples, queue, visited):\n",
    "    # Helper function to add a triple for the parent component and add it to the queue\n",
    "    if parent is not None and parent.uuid not in visited:\n",
    "        triples.append((parent.xtype, parent.uuid, parent.owner.uuid, parent.name, \"is_performed_by\", parent.owner.name))\n",
    "        queue.append(parent.owner)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'root_component' is your starting component\n",
    "# and 'max_triples' is the maximum number of triples you want to insert\n",
    "#results = get_neighbors_component_bfs(root_component, max_triples=50)\n",
    "#root_component = model_capella.la.all_components.by_name(names[top_results[1][0]])\n",
    "\n",
    "entities = [\"Structure, Shielding, & Storage\"]#,\"Waste Management\"]\n",
    "results = []\n",
    "for entity in entities:\n",
    "    root_component = model_capella.la.all_components.by_name(entity)\n",
    "    temp = get_neighbors_component_bfs(root_component, max_triples=25)\n",
    "\n",
    "    for res in temp:\n",
    "        results.append(res)\n",
    "\n",
    "definition = \"\"\n",
    "for triples in results: \n",
    "    definition += \"|\" + triples[3] +\"|\" +\" \" + \"|\" + triples[4] +\"|\" +\" \" +\"|\"+ triples[5] +\"|\" + \"\\n\"\n",
    "\n",
    "#print(definition) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "Wide Field Imagery sensor#1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(res)\n\u001b[1;32m     21\u001b[0m definition \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m triples \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresults\u001b[49m: \n\u001b[1;32m     23\u001b[0m     definition \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m triples[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m triples[\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m triples[\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('org.polarsys.capella.core.data.la:LogicalFunction',\n",
       "  '60fec714-d594-49ae-9a55-8608c0ebc77c',\n",
       "  '2f60ca8e-200e-420a-977a-5dda7c420641',\n",
       "  'Resupply Resources',\n",
       "  'is_performed_by',\n",
       "  'Lander'),\n",
       " ('org.polarsys.capella.core.data.la:LogicalComponent',\n",
       "  '3585479a-6de3-49b0-ab88-787e38c40d36',\n",
       "  '9e7257e0-56d7-4195-a9d0-79fbe540dd5c',\n",
       "  'Logical System',\n",
       "  'directly_contains',\n",
       "  'Water Loop'),\n",
       " ('org.polarsys.capella.core.data.la:LogicalComponent',\n",
       "  '3585479a-6de3-49b0-ab88-787e38c40d36',\n",
       "  '14843f50-4b66-41ac-adad-b5df6db6cd21',\n",
       "  'Logical System',\n",
       "  'directly_contains',\n",
       "  'Air Loop'),\n",
       " ('org.polarsys.capella.core.data.la:LogicalComponent',\n",
       "  '3585479a-6de3-49b0-ab88-787e38c40d36',\n",
       "  '9a8dc1a1-6edc-4804-ab8f-fbfd2534b8bd',\n",
       "  'Logical System',\n",
       "  'directly_contains',\n",
       "  'Thermal Control'),\n",
       " ('org.polarsys.capella.core.data.la:LogicalComponent',\n",
       "  '3585479a-6de3-49b0-ab88-787e38c40d36',\n",
       "  'a33886c4-efe7-44e6-aaae-a37b6704118f',\n",
       "  'Logical System',\n",
       "  'directly_contains',\n",
       "  'Food Loop')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1c807086-920c-401c-9165-a1874e465a8c',\n",
       " '3585479a-6de3-49b0-ab88-787e38c40d36',\n",
       " '69a2d84b-6907-45f4-8a13-11b83afa87e4'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = [res[1] for res in results if res[0] == \"org.polarsys.capella.core.data.la:LogicalComponent\"]\n",
    "set(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Habitation| |is_contained_by| |Logical System|\n",
      "|Habitation| |directly_performs| |Distribute Power|\n",
      "|Distribute Power| |Transmit Power| |Distribute Power|\n",
      "|Distribute Power| |is_performed_by| |Power|\n",
      "|Habitation| |directly_performs| |Provide Sleeping Quarters|\n",
      "|Provide Sleeping Quarters| |Move to Quarters| |Execute Work|\n",
      "|Execute Work| |is_performed_by| |Crew|\n",
      "|Provide Sleeping Quarters| |Setup| |Duplicate 75|\n",
      "|Duplicate 75| |is_performed_by| |Crew|\n",
      "|Provide Sleeping Quarters| |Go to bed| |Sleep|\n",
      "|Sleep| |is_performed_by| |Crew|\n",
      "|Habitation| |directly_performs| |Provide Exercise Facilities|\n",
      "|Provide Exercise Facilities| |Setup| |Duplicate 75|\n",
      "|Duplicate 75| |is_performed_by| |Crew|\n",
      "|Provide Exercise Facilities| |Move to Gym| |Exercise|\n",
      "|Exercise| |is_performed_by| |Crew|\n",
      "|Habitation| |directly_performs| |Provide Medical Services|\n",
      "|Provide Medical Services| |Move to Medical Station| |Perform Medical Care|\n",
      "|Perform Medical Care| |is_performed_by| |Crew|\n",
      "|Provide Medical Services| |Setup| |Duplicate 75|\n",
      "|Duplicate 75| |is_performed_by| |Crew|\n",
      "|Habitation| |directly_performs| |Kitchen|\n",
      "|Kitchen| |Cook Food| |Prepare Food|\n",
      "|Prepare Food| |is_performed_by| |Crew|\n",
      "|Kitchen| |Setup| |Duplicate 75|\n",
      "|Duplicate 75| |is_performed_by| |Crew|\n",
      "|Kitchen| |Bring food to Galley| |Consume Food|\n",
      "|Consume Food| |is_performed_by| |Crew|\n",
      "|Kitchen| |Generate Heat| |Control Temperature|\n",
      "|Control Temperature| |is_performed_by| |Thermal Control|\n",
      "|Crew| |is_contained_by| |Structure|\n",
      "|Crew| |directly_performs| |Execute Work|\n",
      "|Execute Work| |Exit Settlement| |Depressurize|\n",
      "|Depressurize| |is_performed_by| |Airlock|\n",
      "|Execute Work| |Crew Exits Rover| |Drive|\n",
      "|Drive| |is_performed_by| |Surface Robotics|\n",
      "|Execute Work| |Move to Workstation| |Make Decisions|\n",
      "|Make Decisions| |is_performed_by| |Crew|\n",
      "|Execute Work| |Plan Work| |Make Decisions|\n",
      "|Make Decisions| |is_performed_by| |Crew|\n",
      "|Execute Work| |Enter Settlement| |Re-Pressurize|\n",
      "|Re-Pressurize| |is_performed_by| |Airlock|\n",
      "|Execute Work| |Crew Returns to Rover| |Drive|\n",
      "|Drive| |is_performed_by| |Surface Robotics|\n",
      "|Execute Work| |Data Sent to Computer| |Receive Data|\n",
      "|Receive Data| |is_performed_by| |Communications|\n",
      "|Execute Work| |Move to Quarters| |Provide Sleeping Quarters|\n",
      "|Provide Sleeping Quarters| |is_performed_by| |Habitation|\n",
      "|Execute Work| |Generate Heat| |Control Temperature|\n",
      "|Control Temperature| |is_performed_by| |Thermal Control|\n",
      "|Crew| |directly_performs| |Prepare for EVA|\n",
      "|Prepare for EVA| |Enter Airlock| |Depressurize|\n",
      "|Depressurize| |is_performed_by| |Airlock|\n",
      "|Prepare for EVA| |Transfer to Rover| |Depressurize|\n",
      "|Depressurize| |is_performed_by| |Pressurized Rover|\n",
      "|Crew| |directly_performs| |Sleep|\n",
      "|Sleep| |Go to bed| |Provide Sleeping Quarters|\n",
      "|Provide Sleeping Quarters| |is_performed_by| |Habitation|\n",
      "|Crew| |directly_performs| |Prepare Food|\n",
      "|Prepare Food| |Harvest Food| |Produce Food|\n",
      "|Produce Food| |is_performed_by| |Food Loop|\n",
      "|Prepare Food| |Discard Packaging| |Compact Packaging Material|\n",
      "|Compact Packaging Material| |is_performed_by| |Waste Management|\n",
      "|Prepare Food| |Cook Food| |Kitchen|\n",
      "|Kitchen| |is_performed_by| |Habitation|\n",
      "|Crew| |directly_performs| |Hygiene (Shower, Teeth, etc.)|\n",
      "|Hygiene (Shower, Teeth, etc.)| |Clean up| |Provide Hygiene Facilities|\n",
      "|Provide Hygiene Facilities| |is_performed_by| |Water Loop|\n",
      "|Hygiene (Shower, Teeth, etc.)| |Collect Greywater| |Recover Water from Waste|\n",
      "|Recover Water from Waste| |is_performed_by| |Water Loop|\n",
      "|Hygiene (Shower, Teeth, etc.)| |Discard Packaging| |Compact Packaging Material|\n",
      "|Compact Packaging Material| |is_performed_by| |Waste Management|\n",
      "|Hygiene (Shower, Teeth, etc.)| |Collect Water Vapor| |Control Humidity|\n",
      "|Control Humidity| |is_performed_by| |Water Loop|\n",
      "|Crew| |directly_performs| |Consume Oxygen|\n",
      "|Consume Oxygen| |Transport O2 to Crew| |Generate Oxygen|\n",
      "|Generate Oxygen| |is_performed_by| |Air Loop|\n",
      "|Consume Oxygen| |Metabolism| |Exhale CO2 and Water Vapor (incl. sweat)|\n",
      "|Exhale CO2 and Water Vapor (incl. sweat)| |is_performed_by| |Crew|\n",
      "|Crew| |directly_performs| |Consume Water|\n",
      "|Consume Water| |Dispense Water| |Recover Water from Waste|\n",
      "|Recover Water from Waste| |is_performed_by| |Water Loop|\n",
      "|Consume Water| |Metabolism| |Urinate|\n",
      "|Urinate| |is_performed_by| |Crew|\n",
      "|Crew| |directly_performs| |Consume Food|\n",
      "|Consume Food| |Bring food to Galley| |Kitchen|\n",
      "|Kitchen| |is_performed_by| |Habitation|\n",
      "|Consume Food| |Metabolism| |Excrete Feces and Produce Other Solid Waste (e.g. hair, nail clippings)|\n",
      "|Excrete Feces and Produce Other Solid Waste (e.g. hair, nail clippings)| |is_performed_by| |Crew|\n",
      "|Consume Food| |Brush Teeth| |Provide Hygiene Facilities|\n",
      "|Provide Hygiene Facilities| |is_performed_by| |Water Loop|\n",
      "|Crew| |directly_performs| |Exercise|\n",
      "|Exercise| |Move to Gym| |Provide Exercise Facilities|\n",
      "|Provide Exercise Facilities| |is_performed_by| |Habitation|\n",
      "|Exercise| |Soil Clothing| |Remove Dirty Clothing|\n",
      "|Remove Dirty Clothing| |is_performed_by| |Crew|\n",
      "|Exercise| |To Shower| |Provide Hygiene Facilities|\n",
      "|Provide Hygiene Facilities| |is_performed_by| |Water Loop|\n",
      "|Crew| |directly_performs| |Exhale CO2 and Water Vapor (incl. sweat)|\n",
      "|Exhale CO2 and Water Vapor (incl. sweat)| |Metabolism| |Consume Oxygen|\n",
      "|Consume Oxygen| |is_performed_by| |Crew|\n",
      "|Exhale CO2 and Water Vapor (incl. sweat)| |CO2 Collected| |Recover CO2 into O2|\n",
      "|Recover CO2 into O2| |is_performed_by| |Air Loop|\n",
      "|Exhale CO2 and Water Vapor (incl. sweat)| |Collect Water Vapor| |Control Humidity|\n",
      "|Control Humidity| |is_performed_by| |Water Loop|\n",
      "|Crew| |directly_performs| |Urinate|\n",
      "|Urinate| |Metabolism| |Consume Water|\n",
      "|Consume Water| |is_performed_by| |Crew|\n",
      "|Urinate| |Collect Urine| |Recover Water from Waste|\n",
      "|Recover Water from Waste| |is_performed_by| |Water Loop|\n",
      "|Crew| |directly_performs| |Excrete Feces and Produce Other Solid Waste (e.g. hair, nail clippings)|\n",
      "|Excrete Feces and Produce Other Solid Waste (e.g. hair, nail clippings)| |Metabolism| |Consume Food|\n",
      "|Consume Food| |is_performed_by| |Crew|\n",
      "|Excrete Feces and Produce Other Solid Waste (e.g. hair, nail clippings)| |Collect Waste| |Recover Nutrients from Solid Waste|\n",
      "|Recover Nutrients from Solid Waste| |is_performed_by| |Food Loop|\n",
      "|Crew| |directly_performs| |Make Decisions|\n",
      "|Make Decisions| |Data Relayed to Crew| |Transmit Data|\n",
      "|Transmit Data| |is_performed_by| |Communications|\n",
      "|Make Decisions| |Send Commands| |Receive Data|\n",
      "|Receive Data| |is_performed_by| |Communications|\n",
      "|Make Decisions| |Move to Workstation| |Execute Work|\n",
      "|Execute Work| |is_performed_by| |Crew|\n",
      "|Make Decisions| |Plan Work| |Execute Work|\n",
      "|Execute Work| |is_performed_by| |Crew|\n",
      "|Crew| |directly_performs| |Evacuate to Another Module|\n",
      "|Evacuate to Another Module| |Escalate| |Split 68|\n",
      "|Split 68| |is_performed_by| |Crew|\n",
      "|Evacuate to Another Module| |Escalate| |Split 69|\n",
      "|Split 69| |is_performed_by| |Crew|\n",
      "|Evacuate to Another Module| |Escalate| |Split 70|\n",
      "|Split 70| |is_performed_by| |Crew|\n",
      "|Evacuate to Another Module| |Emergency Exit| |Isolate Module|\n",
      "|Isolate Module| |is_performed_by| |Emergency System|\n",
      "|Crew| |directly_performs| |Perform Medical Care|\n",
      "|Perform Medical Care| |Move to Medical Station| |Provide Medical Services|\n",
      "|Provide Medical Services| |is_performed_by| |Habitation|\n",
      "|Crew| |directly_performs| |Remove Dirty Clothing|\n",
      "|Remove Dirty Clothing| |Soil Clothing| |Exercise|\n",
      "|Exercise| |is_performed_by| |Crew|\n",
      "|Remove Dirty Clothing| |Put Clothes in Wash| |Wash Clothing|\n",
      "|Wash Clothing| |is_performed_by| |Water Loop|\n",
      "|Crew| |directly_performs| |Put on Clean Clothing|\n",
      "|Put on Clean Clothing| |Take back clothing| |Wash Clothing|\n",
      "|Wash Clothing| |is_performed_by| |Water Loop|\n",
      "|Crew| |directly_performs| |Don gas masks|\n",
      "|Don gas masks| |Raise Toxic Spill Alarm| |Detect Toxic Spill|\n",
      "|Detect Toxic Spill| |is_performed_by| |Emergency System|\n",
      "|Don gas masks| |Raise Leak Alarm| |Detect Pressure Leak|\n",
      "|Detect Pressure Leak| |is_performed_by| |Emergency System|\n",
      "|Don gas masks| |Move to Spill| |Close & Isolate Module|\n",
      "|Close & Isolate Module| |is_performed_by| |Crew|\n",
      "|Crew| |directly_performs| |Light Oxygen Candle|\n",
      "|Crew| |directly_performs| |Contain Spill or Leak|\n",
      "|Contain Spill or Leak| |Clean Up| |Close & Isolate Module|\n",
      "|Close & Isolate Module| |is_performed_by| |Crew|\n",
      "|Contain Spill or Leak| |Assess Situation| |Split 68|\n",
      "|Split 68| |is_performed_by| |Crew|\n",
      "|Crew| |directly_performs| |Shelter-In-Place|\n",
      "|Shelter-In-Place| |Radiation Warning| |Solar Event Warning|\n",
      "|Solar Event Warning| |is_performed_by| |Emergency System|\n",
      "|Shelter-In-Place| |Assess Situation| |Split 69|\n",
      "|Split 69| |is_performed_by| |Crew|\n",
      "|Crew| |directly_performs| |Close & Isolate Module|\n",
      "|Close & Isolate Module| |Move to Spill| |Don gas masks|\n",
      "|Don gas masks| |is_performed_by| |Crew|\n",
      "|Close & Isolate Module| |Raise Fire Alarm| |Detect Fire|\n",
      "|Detect Fire| |is_performed_by| |Emergency System|\n",
      "|Close & Isolate Module| |Clean Up| |Contain Spill or Leak|\n",
      "|Contain Spill or Leak| |is_performed_by| |Crew|\n",
      "|Close & Isolate Module| |Assess Situation| |Split 70|\n",
      "|Split 70| |is_performed_by| |Crew|\n",
      "|Crew| |directly_performs| |Return to Normal|\n",
      "|Return to Normal| |Return to Normal| |Split 68|\n",
      "|Split 68| |is_performed_by| |Crew|\n",
      "|Return to Normal| |Return to Normal| |Split 69|\n",
      "|Split 69| |is_performed_by| |Crew|\n",
      "|Return to Normal| |Return to Normal| |Split 70|\n",
      "|Split 70| |is_performed_by| |Crew|\n",
      "|Return to Normal| |Reset Alarms| |Return to Normal|\n",
      "|Return to Normal| |is_performed_by| |Emergency System|\n",
      "|Crew| |directly_performs| |Split 68|\n",
      "|Split 68| |Assess Situation| |Contain Spill or Leak|\n",
      "|Contain Spill or Leak| |is_performed_by| |Crew|\n",
      "|Split 68| |Return to Normal| |Return to Normal|\n",
      "|Return to Normal| |is_performed_by| |Crew|\n",
      "|Split 68| |Escalate| |Evacuate to Another Module|\n",
      "|Evacuate to Another Module| |is_performed_by| |Crew|\n",
      "|Crew| |directly_performs| |Split 69|\n",
      "|Split 69| |Assess Situation| |Shelter-In-Place|\n",
      "|Shelter-In-Place| |is_performed_by| |Crew|\n",
      "|Split 69| |Escalate| |Evacuate to Another Module|\n",
      "|Evacuate to Another Module| |is_performed_by| |Crew|\n",
      "|Split 69| |Return to Normal| |Return to Normal|\n",
      "|Return to Normal| |is_performed_by| |Crew|\n",
      "|Crew| |directly_performs| |Split 70|\n",
      "|Split 70| |Assess Situation| |Close & Isolate Module|\n",
      "|Close & Isolate Module| |is_performed_by| |Crew|\n",
      "|Split 70| |Return to Normal| |Return to Normal|\n",
      "|Return to Normal| |is_performed_by| |Crew|\n",
      "|Split 70| |Escalate| |Evacuate to Another Module|\n",
      "|Evacuate to Another Module| |is_performed_by| |Crew|\n",
      "|Crew| |directly_performs| |Set up Habitat|\n",
      "|Set up Habitat| |C&SU| |Re-Pressurize|\n",
      "|Re-Pressurize| |is_performed_by| |Airlock|\n",
      "|Set up Habitat| |Setup| |Duplicate 75|\n",
      "|Duplicate 75| |is_performed_by| |Crew|\n",
      "|Crew| |directly_performs| |Duplicate 75|\n",
      "|Duplicate 75| |Setup| |Set up Habitat|\n",
      "|Set up Habitat| |is_performed_by| |Crew|\n",
      "|Duplicate 75| |Setup| |Kitchen|\n",
      "|Kitchen| |is_performed_by| |Habitation|\n",
      "|Duplicate 75| |Setup| |Provide Exercise Facilities|\n",
      "|Provide Exercise Facilities| |is_performed_by| |Habitation|\n",
      "|Duplicate 75| |Setup| |Provide Sleeping Quarters|\n",
      "|Provide Sleeping Quarters| |is_performed_by| |Habitation|\n",
      "|Duplicate 75| |Setup| |Provide Medical Services|\n",
      "|Provide Medical Services| |is_performed_by| |Habitation|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"|Habitation| |is_contained_by| |Logical System|\\n|Habitation| |directly_performs| |Distribute Power|\\n|Distribute Power| |Transmit Power| |Distribute Power|\\n|Distribute Power| |is_performed_by| |Power|\\n|Habitation| |directly_performs| |Provide Sleeping Quarters|\\n|Provide Sleeping Quarters| |Move to Quarters| |Execute Work|\\n|Execute Work| |is_performed_by| |Crew|\\n|Provide Sleeping Quarters| |Setup| |Duplicate 75|\\n|Duplicate 75| |is_performed_by| |Crew|\\n|Provide Sleeping Quarters| |Go to bed| |Sleep|\\n|Sleep| |is_performed_by| |Crew|\\n|Habitation| |directly_performs| |Provide Exercise Facilities|\\n|Provide Exercise Facilities| |Setup| |Duplicate 75|\\n|Duplicate 75| |is_performed_by| |Crew|\\n|Provide Exercise Facilities| |Move to Gym| |Exercise|\\n|Exercise| |is_performed_by| |Crew|\\n|Habitation| |directly_performs| |Provide Medical Services|\\n|Provide Medical Services| |Move to Medical Station| |Perform Medical Care|\\n|Perform Medical Care| |is_performed_by| |Crew|\\n|Provide Medical Services| |Setup| |Duplicate 75|\\n|Duplicate 75| |is_performed_by| |Crew|\\n|Habitation| |directly_performs| |Kitchen|\\n|Kitchen| |Cook Food| |Prepare Food|\\n|Prepare Food| |is_performed_by| |Crew|\\n|Kitchen| |Setup| |Duplicate 75|\\n|Duplicate 75| |is_performed_by| |Crew|\\n|Kitchen| |Bring food to Galley| |Consume Food|\\n|Consume Food| |is_performed_by| |Crew|\\n|Kitchen| |Generate Heat| |Control Temperature|\\n|Control Temperature| |is_performed_by| |Thermal Control|\\n|Crew| |is_contained_by| |Structure|\\n|Crew| |directly_performs| |Execute Work|\\n|Execute Work| |Exit Settlement| |Depressurize|\\n|Depressurize| |is_performed_by| |Airlock|\\n|Execute Work| |Crew Exits Rover| |Drive|\\n|Drive| |is_performed_by| |Surface Robotics|\\n|Execute Work| |Move to Workstation| |Make Decisions|\\n|Make Decisions| |is_performed_by| |Crew|\\n|Execute Work| |Plan Work| |Make Decisions|\\n|Make Decisions| |is_performed_by| |Crew|\\n|Execute Work| |Enter Settlement| |Re-Pressurize|\\n|Re-Pressurize| |is_performed_by| |Airlock|\\n|Execute Work| |Crew Returns to Rover| |Drive|\\n|Drive| |is_performed_by| |Surface Robotics|\\n|Execute Work| |Data Sent to Computer| |Receive Data|\\n|Receive Data| |is_performed_by| |Communications|\\n|Execute Work| |Move to Quarters| |Provide Sleeping Quarters|\\n|Provide Sleeping Quarters| |is_performed_by| |Habitation|\\n|Execute Work| |Generate Heat| |Control Temperature|\\n|Control Temperature| |is_performed_by| |Thermal Control|\\n|Crew| |directly_performs| |Prepare for EVA|\\n|Prepare for EVA| |Enter Airlock| |Depressurize|\\n|Depressurize| |is_performed_by| |Airlock|\\n|Prepare for EVA| |Transfer to Rover| |Depressurize|\\n|Depressurize| |is_performed_by| |Pressurized Rover|\\n|Crew| |directly_performs| |Sleep|\\n|Sleep| |Go to bed| |Provide Sleeping Quarters|\\n|Provide Sleeping Quarters| |is_performed_by| |Habitation|\\n|Crew| |directly_performs| |Prepare Food|\\n|Prepare Food| |Harvest Food| |Produce Food|\\n|Produce Food| |is_performed_by| |Food Loop|\\n|Prepare Food| |Discard Packaging| |Compact Packaging Material|\\n|Compact Packaging Material| |is_performed_by| |Waste Management|\\n|Prepare Food| |Cook Food| |Kitchen|\\n|Kitchen| |is_performed_by| |Habitation|\\n|Crew| |directly_performs| |Hygiene (Shower, Teeth, etc.)|\\n|Hygiene (Shower, Teeth, etc.)| |Clean up| |Provide Hygiene Facilities|\\n|Provide Hygiene Facilities| |is_performed_by| |Water Loop|\\n|Hygiene (Shower, Teeth, etc.)| |Collect Greywater| |Recover Water from Waste|\\n|Recover Water from Waste| |is_performed_by| |Water Loop|\\n|Hygiene (Shower, Teeth, etc.)| |Discard Packaging| |Compact Packaging Material|\\n|Compact Packaging Material| |is_performed_by| |Waste Management|\\n|Hygiene (Shower, Teeth, etc.)| |Collect Water Vapor| |Control Humidity|\\n|Control Humidity| |is_performed_by| |Water Loop|\\n|Crew| |directly_performs| |Consume Oxygen|\\n|Consume Oxygen| |Transport O2 to Crew| |Generate Oxygen|\\n|Generate Oxygen| |is_performed_by| |Air Loop|\\n|Consume Oxygen| |Metabolism| |Exhale CO2 and Water Vapor (incl. sweat)|\\n|Exhale CO2 and Water Vapor (incl. sweat)| |is_performed_by| |Crew|\\n|Crew| |directly_performs| |Consume Water|\\n|Consume Water| |Dispense Water| |Recover Water from Waste|\\n|Recover Water from Waste| |is_performed_by| |Water Loop|\\n|Consume Water| |Metabolism| |Urinate|\\n|Urinate| |is_performed_by| |Crew|\\n|Crew| |directly_performs| |Consume Food|\\n|Consume Food| |Bring food to Galley| |Kitchen|\\n|Kitchen| |is_performed_by| |Habitation|\\n|Consume Food| |Metabolism| |Excrete Feces and Produce Other Solid Waste (e.g. hair, nail clippings)|\\n|Excrete Feces and Produce Other Solid Waste (e.g. hair, nail clippings)| |is_performed_by| |Crew|\\n|Consume Food| |Brush Teeth| |Provide Hygiene Facilities|\\n|Provide Hygiene Facilities| |is_performed_by| |Water Loop|\\n|Crew| |directly_performs| |Exercise|\\n|Exercise| |Move to Gym| |Provide Exercise Facilities|\\n|Provide Exercise Facilities| |is_performed_by| |Habitation|\\n|Exercise| |Soil Clothing| |Remove Dirty Clothing|\\n|Remove Dirty Clothing| |is_performed_by| |Crew|\\n|Exercise| |To Shower| |Provide Hygiene Facilities|\\n|Provide Hygiene Facilities| |is_performed_by| |Water Loop|\\n|Crew| |directly_performs| |Exhale CO2 and Water Vapor (incl. sweat)|\\n|Exhale CO2 and Water Vapor (incl. sweat)| |Metabolism| |Consume Oxygen|\\n|Consume Oxygen| |is_performed_by| |Crew|\\n|Exhale CO2 and Water Vapor (incl. sweat)| |CO2 Collected| |Recover CO2 into O2|\\n|Recover CO2 into O2| |is_performed_by| |Air Loop|\\n|Exhale CO2 and Water Vapor (incl. sweat)| |Collect Water Vapor| |Control Humidity|\\n|Control Humidity| |is_performed_by| |Water Loop|\\n|Crew| |directly_performs| |Urinate|\\n|Urinate| |Metabolism| |Consume Water|\\n|Consume Water| |is_performed_by| |Crew|\\n|Urinate| |Collect Urine| |Recover Water from Waste|\\n|Recover Water from Waste| |is_performed_by| |Water Loop|\\n|Crew| |directly_performs| |Excrete Feces and Produce Other Solid Waste (e.g. hair, nail clippings)|\\n|Excrete Feces and Produce Other Solid Waste (e.g. hair, nail clippings)| |Metabolism| |Consume Food|\\n|Consume Food| |is_performed_by| |Crew|\\n|Excrete Feces and Produce Other Solid Waste (e.g. hair, nail clippings)| |Collect Waste| |Recover Nutrients from Solid Waste|\\n|Recover Nutrients from Solid Waste| |is_performed_by| |Food Loop|\\n|Crew| |directly_performs| |Make Decisions|\\n|Make Decisions| |Data Relayed to Crew| |Transmit Data|\\n|Transmit Data| |is_performed_by| |Communications|\\n|Make Decisions| |Send Commands| |Receive Data|\\n|Receive Data| |is_performed_by| |Communications|\\n|Make Decisions| |Move to Workstation| |Execute Work|\\n|Execute Work| |is_performed_by| |Crew|\\n|Make Decisions| |Plan Work| |Execute Work|\\n|Execute Work| |is_performed_by| |Crew|\\n|Crew| |directly_performs| |Evacuate to Another Module|\\n|Evacuate to Another Module| |Escalate| |Split 68|\\n|Split 68| |is_performed_by| |Crew|\\n|Evacuate to Another Module| |Escalate| |Split 69|\\n|Split 69| |is_performed_by| |Crew|\\n|Evacuate to Another Module| |Escalate| |Split 70|\\n|Split 70| |is_performed_by| |Crew|\\n|Evacuate to Another Module| |Emergency Exit| |Isolate Module|\\n|Isolate Module| |is_performed_by| |Emergency System|\\n|Crew| |directly_performs| |Perform Medical Care|\\n|Perform Medical Care| |Move to Medical Station| |Provide Medical Services|\\n|Provide Medical Services| |is_performed_by| |Habitation|\\n|Crew| |directly_performs| |Remove Dirty Clothing|\\n|Remove Dirty Clothing| |Soil Clothing| |Exercise|\\n|Exercise| |is_performed_by| |Crew|\\n|Remove Dirty Clothing| |Put Clothes in Wash| |Wash Clothing|\\n|Wash Clothing| |is_performed_by| |Water Loop|\\n|Crew| |directly_performs| |Put on Clean Clothing|\\n|Put on Clean Clothing| |Take back clothing| |Wash Clothing|\\n|Wash Clothing| |is_performed_by| |Water Loop|\\n|Crew| |directly_performs| |Don gas masks|\\n|Don gas masks| |Raise Toxic Spill Alarm| |Detect Toxic Spill|\\n|Detect Toxic Spill| |is_performed_by| |Emergency System|\\n|Don gas masks| |Raise Leak Alarm| |Detect Pressure Leak|\\n|Detect Pressure Leak| |is_performed_by| |Emergency System|\\n|Don gas masks| |Move to Spill| |Close & Isolate Module|\\n|Close & Isolate Module| |is_performed_by| |Crew|\\n|Crew| |directly_performs| |Light Oxygen Candle|\\n|Crew| |directly_performs| |Contain Spill or Leak|\\n|Contain Spill or Leak| |Clean Up| |Close & Isolate Module|\\n|Close & Isolate Module| |is_performed_by| |Crew|\\n|Contain Spill or Leak| |Assess Situation| |Split 68|\\n|Split 68| |is_performed_by| |Crew|\\n|Crew| |directly_performs| |Shelter-In-Place|\\n|Shelter-In-Place| |Radiation Warning| |Solar Event Warning|\\n|Solar Event Warning| |is_performed_by| |Emergency System|\\n|Shelter-In-Place| |Assess Situation| |Split 69|\\n|Split 69| |is_performed_by| |Crew|\\n|Crew| |directly_performs| |Close & Isolate Module|\\n|Close & Isolate Module| |Move to Spill| |Don gas masks|\\n|Don gas masks| |is_performed_by| |Crew|\\n|Close & Isolate Module| |Raise Fire Alarm| |Detect Fire|\\n|Detect Fire| |is_performed_by| |Emergency System|\\n|Close & Isolate Module| |Clean Up| |Contain Spill or Leak|\\n|Contain Spill or Leak| |is_performed_by| |Crew|\\n|Close & Isolate Module| |Assess Situation| |Split 70|\\n|Split 70| |is_performed_by| |Crew|\\n|Crew| |directly_performs| |Return to Normal|\\n|Return to Normal| |Return to Normal| |Split 68|\\n|Split 68| |is_performed_by| |Crew|\\n|Return to Normal| |Return to Normal| |Split 69|\\n|Split 69| |is_performed_by| |Crew|\\n|Return to Normal| |Return to Normal| |Split 70|\\n|Split 70| |is_performed_by| |Crew|\\n|Return to Normal| |Reset Alarms| |Return to Normal|\\n|Return to Normal| |is_performed_by| |Emergency System|\\n|Crew| |directly_performs| |Split 68|\\n|Split 68| |Assess Situation| |Contain Spill or Leak|\\n|Contain Spill or Leak| |is_performed_by| |Crew|\\n|Split 68| |Return to Normal| |Return to Normal|\\n|Return to Normal| |is_performed_by| |Crew|\\n|Split 68| |Escalate| |Evacuate to Another Module|\\n|Evacuate to Another Module| |is_performed_by| |Crew|\\n|Crew| |directly_performs| |Split 69|\\n|Split 69| |Assess Situation| |Shelter-In-Place|\\n|Shelter-In-Place| |is_performed_by| |Crew|\\n|Split 69| |Escalate| |Evacuate to Another Module|\\n|Evacuate to Another Module| |is_performed_by| |Crew|\\n|Split 69| |Return to Normal| |Return to Normal|\\n|Return to Normal| |is_performed_by| |Crew|\\n|Crew| |directly_performs| |Split 70|\\n|Split 70| |Assess Situation| |Close & Isolate Module|\\n|Close & Isolate Module| |is_performed_by| |Crew|\\n|Split 70| |Return to Normal| |Return to Normal|\\n|Return to Normal| |is_performed_by| |Crew|\\n|Split 70| |Escalate| |Evacuate to Another Module|\\n|Evacuate to Another Module| |is_performed_by| |Crew|\\n|Crew| |directly_performs| |Set up Habitat|\\n|Set up Habitat| |C&SU| |Re-Pressurize|\\n|Re-Pressurize| |is_performed_by| |Airlock|\\n|Set up Habitat| |Setup| |Duplicate 75|\\n|Duplicate 75| |is_performed_by| |Crew|\\n|Crew| |directly_performs| |Duplicate 75|\\n|Duplicate 75| |Setup| |Set up Habitat|\\n|Set up Habitat| |is_performed_by| |Crew|\\n|Duplicate 75| |Setup| |Kitchen|\\n|Kitchen| |is_performed_by| |Habitation|\\n|Duplicate 75| |Setup| |Provide Exercise Facilities|\\n|Provide Exercise Facilities| |is_performed_by| |Habitation|\\n|Duplicate 75| |Setup| |Provide Sleeping Quarters|\\n|Provide Sleeping Quarters| |is_performed_by| |Habitation|\\n|Duplicate 75| |Setup| |Provide Medical Services|\\n|Provide Medical Services| |is_performed_by| |Habitation|\\n\"\"\")\n",
    "#model_capella.la.all_functions.by_name(\"Duplicate 75\").output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Split 68 <span style=\"font-size: 70%;\">(org.polarsys.capella.core.data.la:LogicalFunction)</span></h1><table><tr><th style=\"text-align: right;\">applied_property_value_groups</th><td style=\"text-align: left;\"><p><em>(Empty list)</em></p></td></tr><tr><th style=\"text-align: right;\">applied_property_values</th><td style=\"text-align: left;\"><p><em>(Empty list)</em></p></td></tr><tr><th style=\"text-align: right;\">available_in_states</th><td style=\"text-align: left;\"><p><em>(Empty list)</em></p></td></tr><tr><th style=\"text-align: right;\">constraints</th><td style=\"text-align: left;\"><p><em>(Empty list)</em></p></td></tr><tr><th style=\"text-align: right;\">description</th><td style=\"text-align: left;\"></td></tr><tr><th style=\"text-align: right;\">diagrams</th><td style=\"text-align: left;\"><p><em>(Empty list)</em></p></td></tr><tr><th style=\"text-align: right;\">exchanges</th><td style=\"text-align: left;\"><p><em>(Empty list)</em></p></td></tr><tr><th style=\"text-align: right;\">filtering_criteria</th><td style=\"text-align: left;\"><p><em>(Empty list)</em></p></td></tr><tr><th style=\"text-align: right;\">functions</th><td style=\"text-align: left;\"><p><em>(Empty list)</em></p></td></tr><tr><th style=\"text-align: right;\">inputs</th><td style=\"text-align: left;\"><ol start=\"0\" style=\"text-align: left;\"><li><strong>FunctionInputPort</strong> &quot;FIP 1&quot; (724a25e0-277d-4d3f-bfc5-686fc29e2685)</li></ol></td></tr><tr><th style=\"text-align: right;\">is_leaf</th><td style=\"text-align: left;\"><em>True</em></td></tr><tr><th style=\"text-align: right;\">kind</th><td style=\"text-align: left;\"><em>&lt;FunctionKind.SPLIT: 5&gt;</em></td></tr><tr><th style=\"text-align: right;\">name</th><td style=\"text-align: left;\">Split 68</td></tr><tr><th style=\"text-align: right;\">outputs</th><td style=\"text-align: left;\"><ol start=\"0\" style=\"text-align: left;\"><li><strong>FunctionOutputPort</strong> &quot;FOP 1&quot; (a2b0f984-eaaf-4a12-9329-904880b00ff0)</li><li><strong>FunctionOutputPort</strong> &quot;FOP 2&quot; (a64a67ff-ae66-4ce5-a5d3-60b26d599f8e)</li></ol></td></tr><tr><th style=\"text-align: right;\">owner</th><td style=\"text-align: left;\"><em>Backreference to LogicalComponent - omitted: can be slow to compute. Display this property directly to show.</em></td></tr><tr><th style=\"text-align: right;\">packages</th><td style=\"text-align: left;\"><p><em>(Empty list)</em></p></td></tr><tr><th style=\"text-align: right;\">parent</th><td style=\"text-align: left;\"><strong>LogicalFunction</strong> &quot;Reassess Emergency&quot; (e564bead-05ec-449a-865f-904758b095e9)</td></tr><tr><th style=\"text-align: right;\">progress_status</th><td style=\"text-align: left;\">NOT_SET</td></tr><tr><th style=\"text-align: right;\">property_value_groups</th><td style=\"text-align: left;\"><p><em>(Empty list)</em></p></td></tr><tr><th style=\"text-align: right;\">property_values</th><td style=\"text-align: left;\"><p><em>(Empty list)</em></p></td></tr><tr><th style=\"text-align: right;\">realized_functions</th><td style=\"text-align: left;\"><p><em>(Empty list)</em></p></td></tr><tr><th style=\"text-align: right;\">realized_system_functions</th><td style=\"text-align: left;\"><p><em>(Empty list)</em></p></td></tr><tr><th style=\"text-align: right;\">realizing_functions</th><td style=\"text-align: left;\"><em>Backreference to  - omitted: can be slow to compute. Display this property directly to show.</em></td></tr><tr><th style=\"text-align: right;\">realizing_physical_functions</th><td style=\"text-align: left;\"><em>Backreference to PhysicalFunction - omitted: can be slow to compute. Display this property directly to show.</em></td></tr><tr><th style=\"text-align: right;\">related_exchanges</th><td style=\"text-align: left;\"><em>Backreference to FunctionalExchange - omitted: can be slow to compute. Display this property directly to show.</em></td></tr><tr><th style=\"text-align: right;\">requirements</th><td style=\"text-align: left;\"><p><em>(Empty list)</em></p></td></tr><tr><th style=\"text-align: right;\">summary</th><td style=\"text-align: left;\"><em>None</em></td></tr><tr><th style=\"text-align: right;\">traces</th><td style=\"text-align: left;\"><p><em>(Empty list)</em></p></td></tr><tr><th style=\"text-align: right;\">uuid</th><td style=\"text-align: left;\">401414e0-526c-4d17-9642-42bfaa50c4fa</td></tr><tr><th style=\"text-align: right;\">xtype</th><td style=\"text-align: left;\">org.polarsys.capella.core.data.la:LogicalFunction</td></tr></table>"
      ],
      "text/plain": [
       "<LogicalFunction 'Split 68' (401414e0-526c-4d17-9642-42bfaa50c4fa)>\n",
       ".applied_property_value_groups = []\n",
       ".applied_property_values = []\n",
       ".available_in_states = []\n",
       ".constraints = []\n",
       ".description = ''\n",
       ".diagrams = []\n",
       ".exchanges = []\n",
       ".filtering_criteria = []\n",
       ".functions = []\n",
       ".inputs = [0] <FunctionInputPort 'FIP 1' (724a25e0-277d-4d3f-bfc5-686fc29e2685)>\n",
       ".is_leaf = True\n",
       ".kind = <FunctionKind.SPLIT: 5>\n",
       ".name = 'Split 68'\n",
       ".outputs = [0] <FunctionOutputPort 'FOP 1' (a2b0f984-eaaf-4a12-9329-904880b00ff0)>\n",
       "           [1] <FunctionOutputPort 'FOP 2' (a64a67ff-ae66-4ce5-a5d3-60b26d599f8e)>\n",
       ".owner = ... # backreference to LogicalComponent - omitted: can be slow to compute\n",
       ".packages = []\n",
       ".parent = <LogicalFunction 'Reassess Emergency' (e564bead-05ec-449a-865f-904758b095e9)>\n",
       ".progress_status = 'NOT_SET'\n",
       ".property_value_groups = []\n",
       ".property_values = []\n",
       ".realized_functions = []\n",
       ".realized_system_functions = []\n",
       ".realizing_functions = ... # backreference to  - omitted: can be slow to compute\n",
       ".realizing_physical_functions = ... # backreference to PhysicalFunction - omitted: can be slow to compute\n",
       ".related_exchanges = ... # backreference to FunctionalExchange - omitted: can be slow to compute\n",
       ".requirements = []\n",
       ".summary = None\n",
       ".traces = []\n",
       ".uuid = '401414e0-526c-4d17-9642-42bfaa50c4fa'\n",
       ".xtype = 'org.polarsys.capella.core.data.la:LogicalFunction'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_capella.la.all_components.by_name(\"Crew\").allocated_functions[23].outputs[0].owner#.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Structure, Shielding, & Storage| |is_contained_by| |Logical System|\n",
      "|Structure, Shielding, & Storage| |directly_performs| |Protect Against Radiation & Debris|\n",
      "|Structure, Shielding, & Storage| |directly_performs| |Support Weight of Internals + Shielding|\n",
      "|Structure, Shielding, & Storage| |directly_performs| |Store Supplies & Equipment|\n",
      "|Store Supplies & Equipment| |Move Resources from Lander to Habitat| |Resupply Resources|\n",
      "|Resupply Resources| |is_performed_by| |Lander|\n",
      "|Logical System| |directly_contains| |Water Loop|\n",
      "|Logical System| |directly_contains| |Air Loop|\n",
      "|Logical System| |directly_contains| |Thermal Control|\n",
      "|Logical System| |directly_contains| |Food Loop|\n",
      "|Logical System| |directly_contains| |Waste Management|\n",
      "|Logical System| |directly_contains| |Command & Data Handling|\n",
      "|Logical System| |directly_contains| |Communications|\n",
      "|Logical System| |directly_contains| |Emergency System|\n",
      "|Logical System| |directly_contains| |Power|\n",
      "|Logical System| |directly_contains| |Habitation|\n",
      "|Logical System| |directly_contains| |Structure, Shielding, & Storage|\n",
      "|Logical System| |is_contained_by| |Structure|\n",
      "|Lander| |is_contained_by| |Surface Robotics|\n",
      "|Lander| |directly_performs| |Resupply Resources|\n",
      "|Resupply Resources| |Move Resources from Lander to Habitat| |Store Supplies & Equipment|\n",
      "|Store Supplies & Equipment| |is_performed_by| |Structure, Shielding, & Storage|\n",
      "|Lander| |directly_performs| |Crew Exchange|\n",
      "|Crew Exchange| |Enter Settlement| |EVA|\n",
      "|Water Loop| |is_contained_by| |Logical System|\n",
      "|Water Loop| |directly_performs| |Wash Clothing|\n",
      "|Wash Clothing| |Put Clothes in Wash| |Remove Dirty Clothing|\n",
      "|Remove Dirty Clothing| |is_performed_by| |Crew|\n",
      "|Wash Clothing| |Take back clothing| |Put on Clean Clothing|\n",
      "|Put on Clean Clothing| |is_performed_by| |Crew|\n",
      "|Wash Clothing| |Supply Greywater| |Recover Water from Waste|\n",
      "|Recover Water from Waste| |is_performed_by| |Water Loop|\n",
      "|Water Loop| |directly_performs| |Recover Water from Waste|\n",
      "|Recover Water from Waste| |Collect Urine| |Urinate|\n",
      "|Urinate| |is_performed_by| |Crew|\n",
      "|Recover Water from Waste| |Collect Greywater| |Hygiene (Shower, Teeth, etc.)|\n",
      "|Hygiene (Shower, Teeth, etc.)| |is_performed_by| |Crew|\n",
      "|Recover Water from Waste| |Supply Condensate| |Control Humidity|\n",
      "|Control Humidity| |is_performed_by| |Water Loop|\n",
      "|Recover Water from Waste| |Extract Residual Water| |Compact Packaging Material|\n",
      "|Compact Packaging Material| |is_performed_by| |Waste Management|\n",
      "|Recover Water from Waste| |Supply Greywater| |Wash Clothing|\n",
      "|Wash Clothing| |is_performed_by| |Water Loop|\n",
      "|Recover Water from Waste| |Split Water into O2| |Generate Oxygen|\n",
      "|Generate Oxygen| |is_performed_by| |Air Loop|\n",
      "|Recover Water from Waste| |Expel waste brine| |Store / Vent Waste|\n",
      "|Store / Vent Waste| |is_performed_by| |Settlement Peripherals & Payloads|\n",
      "|Recover Water from Waste| |Supply Water| |Produce Food|\n",
      "|Produce Food| |is_performed_by| |Food Loop|\n",
      "|Recover Water from Waste| |Dispense Water| |Consume Water|\n",
      "|Consume Water| |is_performed_by| |Crew|\n",
      "|Water Loop| |directly_performs| |Provide Hygiene Facilities|\n",
      "|Provide Hygiene Facilities| |To Shower| |Exercise|\n",
      "|Exercise| |is_performed_by| |Crew|\n",
      "|Provide Hygiene Facilities| |Brush Teeth| |Consume Food|\n",
      "|Consume Food| |is_performed_by| |Crew|\n",
      "|Provide Hygiene Facilities| |Clean up| |Hygiene (Shower, Teeth, etc.)|\n",
      "|Hygiene (Shower, Teeth, etc.)| |is_performed_by| |Crew|\n",
      "|Water Loop| |directly_performs| |Distribute Power|\n",
      "|Distribute Power| |Supply Power| |Distribute Power|\n",
      "|Distribute Power| |is_performed_by| |Power|\n",
      "|Distribute Power| |Supply Backup Power| |Distribute Backup Power|\n",
      "|Distribute Backup Power| |is_performed_by| |Emergency System|\n",
      "|Water Loop| |directly_performs| |Control Humidity|\n",
      "|Control Humidity| |Collect Water Vapor| |Exhale CO2 and Water Vapor (incl. sweat)|\n",
      "|Exhale CO2 and Water Vapor (incl. sweat)| |is_performed_by| |Crew|\n",
      "|Control Humidity| |Collect Water Vapor| |Hygiene (Shower, Teeth, etc.)|\n",
      "|Hygiene (Shower, Teeth, etc.)| |is_performed_by| |Crew|\n",
      "|Control Humidity| |Supply Condensate| |Recover Water from Waste|\n",
      "|Recover Water from Waste| |is_performed_by| |Water Loop|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(\"\\n\")\n",
    "\n",
    "print(definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86277163cbcd46c4b66b22502013013b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/miniconda3/envs/llm_updated/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM#, # AutoModelForCausalLM, AutoTokenizer#, LlamaForCausalLM, LlamaTokenizer #, OPTForCausalLM, AutoModelForCausalLM\n",
    "import torch \n",
    "MODEL_NAME =\"openchat/openchat_3.5\"#\"lmsys/vicuna-13b-v1.5\"#\"openchat/openchat_3.5\"#\"lmsys/vicuna-13b-v1.5\" #\"mistralai/Mistral-7B-v0.1\"\n",
    "LOAD_8BIT = True \n",
    "# #LlamaForCausalLM.from_pretrained(MODEL_NAME,\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME,\n",
    "        load_in_8bit=LOAD_8BIT,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        )\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if \"open\" in model.config._name_or_path:\n",
    "\n",
    "    model.generation_config.pad_token_id = 0\n",
    "\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    def __init__(self, stops = [], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = [stop.to(\"cuda\") for stop in stops]\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        last_token = input_ids[0][-1]\n",
    "        for stop in self.stops:\n",
    "            if tokenizer.decode(stop) == tokenizer.decode(last_token):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "stop_words = [\"Yes\", \"No\"] #\"\\nYes\"\n",
    "\n",
    "stop_words_ids = [tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze() for stop_word in stop_words]\n",
    "\n",
    "#stop_words_ids = [5592, 5613, 1770]\n",
    "#stop_words_ids = [torch.tensor(id) for id in stop_words_ids]\n",
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template openchat : GPT4 Correct User: Hello<|end_of_turn|>GPT4 Correct Assistant: Hi<|end_of_turn|>GPT4 Correct User: How are you today?<|end_of_turn|>GPT4 Correct Assistant:\n",
    "model.generation_config.pad_token_id = 0 # = GenerationConfig.from_pretrained(\"openchat/openchat_3.5\")\n",
    "test = \"GPT4 Correct User: What is the colour of the sky?<|end_of_turn|>GPT4 Correct Assistant: \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NEW_TOKENS = 512 \n",
    "extra_args = {\"max_new_tokens\":MAX_NEW_TOKENS,\n",
    "          #  \"min_new_tokens\": 256, \n",
    "             \"do_sample\":True,#True, \n",
    "            \"num_beams\" : 1, \n",
    "            \"num_return_sequences\" : 1, \n",
    "            #\"no_repeat_ngram_size\": 12, \n",
    "            \"temperature\": 0.5, \n",
    "            \"top_p\": 0.95,\n",
    "          #  \"begin_suppress_tokens\": [2], \n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "generation_config = GenerationConfig()\n",
    "print(generation_config.do_sample)\n",
    "\n",
    "generation_config = GenerationConfig(**extra_args)\n",
    "print(generation_config.do_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "generation_config = GenerationConfig()\n",
    "print(generation_config.do_sample)\n",
    "print(generation_config.no_repeat_ngram_size)\n",
    "print(generation_config.temperature)\n",
    "print(generation_config.top_p)\n",
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    def __init__(self, stops = [], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = [stop.to(\"cuda\") for stop in stops]\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        last_token = input_ids[0][-1]\n",
    "        for stop in self.stops:\n",
    "            if tokenizer.decode(stop) == tokenizer.decode(last_token):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "stop_words = [\"Yes\", \"No\"] #\"\\nYes\"\n",
    "\n",
    "stop_words_ids = [tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze() for stop_word in stop_words]\n",
    "\n",
    "#stop_words_ids = [5592, 5613, 1770]\n",
    "\n",
    "#stop_words_ids = [torch.tensor(id) for id in stop_words_ids]\n",
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(5592), tensor([28705,    13,  5613]), tensor(1770)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer.vocab#[5592]\n",
    "stop_words_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(stop_words_ids[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(stop_words_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "def evaluate(\n",
    "    prompt,\n",
    "    model = None,\n",
    "    tokenizer = None,\n",
    "    stopping_criteria = None,\n",
    "    device = 'cuda',\n",
    "    **kwargs,\n",
    "):\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    generation_config = GenerationConfig(\n",
    "        **kwargs,\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            output_hidden_states= True,\n",
    "            stopping_criteria= stopping_criteria,\n",
    "            output_attentions=True,\n",
    "        )\n",
    "        s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    return output, generation_output\n",
    "\n",
    "MAX_NEW_TOKENS = 600\n",
    "extra_args = {\"max_new_tokens\":MAX_NEW_TOKENS,\n",
    "          #  \"min_new_tokens\": 256, \n",
    "             \"do_sample\":False,#True, \n",
    "            \"num_beams\" : 1, \n",
    "            \"num_return_sequences\" : 1, \n",
    "            #\"no_repeat_ngram_size\": 12, \n",
    "            \"temperature\": 0.5, \n",
    "            \"top_p\": 0.95,\n",
    "          #  \"begin_suppress_tokens\": [2], \n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = \"What is a TT&C diplexer?\"\n",
    "# output = evaluate(input, model = model, tokenizer= tokenizer, device = \"cuda\", **extra_args) #\n",
    "\n",
    "# print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"Hi\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduced definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red = \"\"\"|Spaceshuttle| |is_contained_by| |Structure|\n",
    "|Spaceshuttle| |Transport Astronauts| |Astronauts|\n",
    "|Astronauts| |is_contained_by| |Structure|\n",
    "|Astronauts| |Transport Astronauts| |Spaceshuttle|\n",
    "|Astronauts| |Services Telescope| |HUBBLE Space Telescope|\n",
    "|Astronauts| |directly_performs| |Service Telescope|\n",
    "|HUBBLE Space Telescope| |directly_contains| |Support System Module|\n",
    "|HUBBLE Space Telescope| |directly_contains| |Instruments module|\n",
    "|HUBBLE Space Telescope| |directly_contains| |Science&Instrument Control&Data Handling module|\n",
    "|HUBBLE Space Telescope| |directly_contains| |Optical Telescope module|\n",
    "|HUBBLE Space Telescope| |is_contained_by| |Structure|\n",
    "|HUBBLE Space Telescope| |has| |Drymass in [KG] 11110.0|\n",
    "|HUBBLE Space Telescope| |has| |Wet mass in [KG] 12218.0|\n",
    "|HUBBLE Space Telescope| |Space network| |Tracking & Data Relay Satellite|\n",
    "|HUBBLE Space Telescope| |Services Telescope| |Astronauts|\n",
    "|HUBBLE Space Telescope| |Light| |Space|\n",
    "|Tracking & Data Relay Satellite| |is_contained_by| |Structure|\n",
    "|Tracking & Data Relay Satellite| |Space network| |HUBBLE Space Telescope|\n",
    "|Tracking & Data Relay Satellite| |directly_performs| |Broadcast information from space|\n",
    "|Broadcast information from space| |Information to earth| |Broadcast signal#1|\n",
    "|Broadcast signal#1| |is_performed_by| |Antenna#1|\n",
    "|Broadcast information from space| |Information to earth| |Broadcast signal#2|\n",
    "|Broadcast signal#2| |is_performed_by| |Antenna#2|\n",
    "|Broadcast information from space| |Information from earth| |Broadcast signal#1|\n",
    "|Broadcast signal#1| |is_performed_by| |Antenna#1|\n",
    "|Broadcast information from space| |Information from earth| |Broadcast signal#2|\n",
    "|Broadcast signal#2| |is_performed_by| |Antenna#2|\n",
    "|Space| |is_contained_by| |Structure|\n",
    "|Space| |Light| |HUBBLE Space Telescope|\n",
    "|Space| |directly_performs| |Diffuse light|\n",
    "|Diffuse light| |Space light| |Collect light|\n",
    "|Collect light| |is_performed_by| |Light collector|\n",
    "|Diffuse light| |Space light| |Find and maintain a lock on guide stars|\n",
    "|Find and maintain a lock on guide stars| |is_performed_by| |Locking sensor|\n",
    "|Diffuse light| |Space light| |Collect sunlight energy#1|\n",
    "|Collect sunlight energy#1| |is_performed_by| |Solar Array#1|\n",
    "|Diffuse light| |Space light| |Collect sunlight energy#2|\n",
    "|Collect sunlight energy#2| |is_performed_by| |Solar Array#2|\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = \"\"\"|Spaceshuttle| |directly_contains| |Deployment Boom |\n",
    "|Spaceshuttle| |is_contained_by| |Structure|\n",
    "|Spaceshuttle| |Transport Astronauts| |Astronauts|\n",
    "|Deployment Boom | |is_contained_by| |Spaceshuttle|\n",
    "|Deployment Boom | |Deployment| |HUBBLE Space Telescope|\n",
    "|Deployment Boom | |Return| |HUBBLE Space Telescope|\n",
    "|Deployment Boom | |directly_performs| |Deploy Telescope|\n",
    "|Deployment Boom | |directly_performs| |Catch satellite|\n",
    "|Structure| |directly_contains| |HUBBLE Space Telescope|\n",
    "|Structure| |directly_contains| |Tracking & Data Relay Satellite|\n",
    "|Structure| |directly_contains| |Space|\n",
    "|Structure| |directly_contains| |Astronauts|\n",
    "|Structure| |directly_contains| |Spaceshuttle|\n",
    "|Astronauts| |is_contained_by| |Structure|\n",
    "|Astronauts| |Transport Astronauts| |Spaceshuttle|\n",
    "|Astronauts| |Services Telescope| |HUBBLE Space Telescope|\n",
    "|Astronauts| |directly_performs| |Service Telescope|\n",
    "|HUBBLE Space Telescope| |directly_contains| |Support System Module|\n",
    "|HUBBLE Space Telescope| |directly_contains| |Instruments module|\n",
    "|HUBBLE Space Telescope| |directly_contains| |Science&Intrument Control&Data Handling module|\n",
    "|HUBBLE Space Telescope| |directly_contains| |Optical Telescope module|\n",
    "|HUBBLE Space Telescope| |is_contained_by| |Structure|\n",
    "|HUBBLE Space Telescope| |has| |Drymass in [KG] 11110.0|\n",
    "|HUBBLE Space Telescope| |has| |Wet mass in [KG] 12218.0|\n",
    "|HUBBLE Space Telescope| |Space network| |Tracking & Data Relay Satellite|\n",
    "|HUBBLE Space Telescope| |Deployment| |Deployment Boom |\n",
    "|HUBBLE Space Telescope| |Return| |Deployment Boom |\n",
    "|HUBBLE Space Telescope| |Services Telescope| |Astronauts|\n",
    "|HUBBLE Space Telescope| |Light| |Space|\n",
    "|Tracking & Data Relay Satellite| |is_contained_by| |Structure|\n",
    "|Tracking & Data Relay Satellite| |Space network| |HUBBLE Space Telescope|\n",
    "|Tracking & Data Relay Satellite| |directly_performs| |Broadcast information from space|\n",
    "|Broadcast information from space| |Information to earth| |Broadcast signal#1|\n",
    "|Broadcast signal#1| |is_performed_by| |Antenna#1|\n",
    "|Broadcast information from space| |Information to earth| |Broadcast signal#2|\n",
    "|Broadcast signal#2| |is_performed_by| |Antenna#2|\n",
    "|Broadcast information from space| |Information from earth| |Broadcast signal#1|\n",
    "|Broadcast signal#1| |is_performed_by| |Antenna#1|\n",
    "|Broadcast information from space| |Information from earth| |Broadcast signal#2|\n",
    "|Broadcast signal#2| |is_performed_by| |Antenna#2|\n",
    "|Space| |is_contained_by| |Structure|\n",
    "|Space| |Light| |HUBBLE Space Telescope|\n",
    "|Space| |directly_performs| |Diffuse light|\n",
    "|Diffuse light| |Space light| |Collect light|\n",
    "|Collect light| |is_performed_by| |Light collector|\n",
    "|Diffuse light| |Space light| |Find and maintain a lock on guide stars|\n",
    "|Find and maintain a lock on guide stars| |is_performed_by| |Locking sensor|\n",
    "|Diffuse light| |Space light| |Collect sunlight energy#1|\n",
    "|Collect sunlight energy#1| |is_performed_by| |Solar Array#1|\n",
    "|Diffuse light| |Space light| |Collect sunlight energy#2|\n",
    "|Collect sunlight energy#2| |is_performed_by| |Solar Array#2|\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### excecute prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/miniconda3/envs/llm_updated/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/paul/miniconda3/envs/llm_updated/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/paul/miniconda3/envs/llm_updated/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "#prompt_reason = \"\"\"Based on the system description, is this requirement satisfied? Include in your answer \"Final Answer: Yes/No\" . Use Python scripting if you need to calculate something by starting with '''python... Let's think step-by-step. \"\"\"\n",
    "prompt_reason = \"\"\"Based on the system description, is this requirement satisfied? Include in your answer \"Final Answer: Yes/No\". Use Python scripting if you need to calculate something by starting with '''python... Let's think step-by-step. \"\"\"\n",
    "\n",
    "# prompt_reason = \"\"\"Based on the system description, is this requirement satisfied?\n",
    "# 1. Read the information (entities and relations) given for the System carefully and identify the triples that are potentially useful in your analysis. \n",
    "# 2. Next try to establish if that information either implicitly or explicitly fulfills the requirement. \n",
    "# 3. State if the requirement is satisfied or not by stating Yes or No.\n",
    "\n",
    "# Answer (Yes or No ONLY): \"\"\"\n",
    "## This could involve performing a gap analysis between the engineering system and the requirement. \n",
    "system_prompt =\"GPT4 Correct Assistant: \"# \"ASSISTANT:\"#\"GPT4 Correct Assistant: \"\n",
    "user_prompt = \"GPT4 Correct User: \"#\"HUMAN:\"#\"GPT4 Correct User: \"\n",
    " \n",
    "\n",
    "# \"\"\"|Spaceshuttle| |is_contained_by| |Structure|\n",
    "# |Spaceshuttle| |Transport Astronauts| |Astronauts|\n",
    "# |Astronauts| |is_contained_by| |Structure|\n",
    "# |Astronauts| |Transport Astronauts| |Spaceshuttle|\n",
    "# |Astronauts| |Services Telescope| |HUBBLE Space Telescope|\n",
    "# |Astronauts| |directly_performs| |Service Telescope|\"\"\"\n",
    "\n",
    "definition = \"\"\"\n",
    "|Astronauts| |Services Telescope| |HUBBLE Space Telescope|\n",
    "|Astronauts| |directly_performs| |Service Telescope|\n",
    "|HUBBLE Space Telescope| |directly_contains| |Support System Module|\n",
    "|HUBBLE Space Telescope| |directly_contains| |Instruments module|\n",
    "|HUBBLE Space Telescope| |directly_contains| |Science&Instrument Control&Data Handling module|\n",
    "|HUBBLE Space Telescope| |directly_contains| |Optical Telescope module|\n",
    "|HUBBLE Space Telescope| |is_contained_by| |Structure|\n",
    "|HUBBLE Space Telescope| |has| |Drymass in [KG] 11110.0|\n",
    "|HUBBLE Space Telescope| |has| |Wet mass in [KG] 12218.0|\n",
    "|HUBBLE Space Telescope| |Space network| |Tracking & Data Relay Satellite|\n",
    "|HUBBLE Space Telescope| |Services Telescope| |Astronauts|\n",
    "|HUBBLE Space Telescope| |Light| |Space|\n",
    "|Tracking & Data Relay Satellite| |is_contained_by| |Structure|\n",
    "|Tracking & Data Relay Satellite| |Space network| |HUBBLE Space Telescope|\n",
    "|Tracking & Data Relay Satellite| |directly_performs| |Broadcast information from space|\n",
    "|Broadcast information from space| |Information to earth| |Broadcast signal#1|\n",
    "|Broadcast signal#1| |is_performed_by| |Antenna#1|\n",
    "|Broadcast information from space| |Information to earth| |Broadcast signal#2|\n",
    "|Broadcast signal#2| |is_performed_by| |Antenna#2|\n",
    "|Broadcast information from space| |Information from earth| |Broadcast signal#1|\n",
    "|Broadcast signal#1| |is_performed_by| |Antenna#1|\n",
    "|Broadcast information from space| |Information from earth| |Broadcast signal#2|\n",
    "|Broadcast signal#2| |is_performed_by| |Antenna#2|\n",
    "|Space| |is_contained_by| |Structure|\n",
    "|Space| |Light| |HUBBLE Space Telescope|\n",
    "|Space| |directly_performs| |Diffuse light|\n",
    "|Diffuse light| |Space light| |Collect light|\n",
    "|Collect light| |is_performed_by| |Light collector|\n",
    "|Diffuse light| |Space light| |Find and maintain a lock on guide stars|\n",
    "|Find and maintain a lock on guide stars| |is_performed_by| |Locking sensor|\n",
    "|Diffuse light| |Space light| |Collect sunlight energy#1|\n",
    "|Collect sunlight energy#1| |is_performed_by| |Solar Array#1|\n",
    "|Diffuse light| |Space light| |Collect sunlight energy#2|\n",
    "|Collect sunlight energy#2| |is_performed_by| |Solar Array#2|\n",
    "\"\"\"\n",
    "definition = \"\"\"|JWST| |isPlacedIn| |Transfer Orbit| by |Launch Vehicle|\n",
    "|JWST| |orbits| |Second LaGrange Point (L2)|\n",
    "|JWST| |maintains_ZExcursion| \"≤500,000 Km\"\n",
    "|JWST| |maintains_YExcursion| \"≤800,000 Km\"\n",
    "|JWST| |avoids| |Eclipse|\n",
    "|JWST| |transfersTo| |L2 Operational Orbit| using |DUA IRD (JWST-IRD-003674)|\n",
    "|JWST| |has_MissionLifetime| \"5 years\"\n",
    "|JWST| |completes_Commissioning| within \"6 months\"\n",
    "|JWST System| |delivers_RealTimeTelemetry| \"92.5%\"\n",
    "|JWST System| |archives_RecordedData| \"97%\"\n",
    "|JWST| |achieves_Sensitivity| per |NIRCam, NIRSpec, MIRI, FGS-TF Calculations|\"\"\"\n",
    "\n",
    "req = \"The Space Shuttle shall provide the basic transportation for all phases of the HST program including deployment, on-orbit servicing, and reboost or return to earth.\"\n",
    "req = \"The planned commissioning phase shall end no later than six months after launch.\"\n",
    "prompt_reason = f\"{user_prompt}System: {definition}\\nRequirement: {req}\\n{prompt_reason}\\n\"#{system_prompt}\"# +\"Let's analyze the given System and requirement step-by-step:\"\n",
    "\n",
    "#prompt_reason = \"What is the colour of the sky?\"\n",
    "#final answer: yes\n",
    "#prompt_reason = \"Please answer the following question. Are there 50 states in the US. Answer (Yes or No ONLY):\"\n",
    "#output, complete_output = evaluate(prompt_reason, model = model, tokenizer= tokenizer, stopping_criteria = stopping_criteria, device = \"cuda\", **extra_args) #\n",
    "output, complete_output = evaluate(prompt_reason, model = model, tokenizer= tokenizer, stopping_criteria = None, device = \"cuda\", **extra_args) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete_output.sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8196"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = (0,{\"scores\": torch.zeros(8196).long()})\n",
    "\n",
    "len(output[1]['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> GPT4 Correct User: System: |JWST| |isPlacedIn| |Transfer Orbit| by |Launch Vehicle|\n",
      "|JWST| |orbits| |Second LaGrange Point (L2)|\n",
      "|JWST| |maintains_ZExcursion| \"≤500,000 Km\"\n",
      "|JWST| |maintains_YExcursion| \"≤800,000 Km\"\n",
      "|JWST| |avoids| |Eclipse|\n",
      "|JWST| |transfersTo| |L2 Operational Orbit| using |DUA IRD (JWST-IRD-003674)|\n",
      "|JWST| |has_MissionLifetime| \"5 years\"\n",
      "|JWST| |completes_Commissioning| within \"6 months\"\n",
      "|JWST System| |delivers_RealTimeTelemetry| \"92.5%\"\n",
      "|JWST System| |archives_RecordedData| \"97%\"\n",
      "|JWST| |achieves_Sensitivity| per |NIRCam, NIRSpec, MIRI, FGS-TF Calculations|\n",
      "Requirement: The planned commissioning phase shall end no later than six months after launch.\n",
      "Based on the system description, is this requirement satisfied? Include in your answer \"Final Answer: Yes/No\". Use Python scripting if you need to calculate something by starting with '''python... Let's think step-by-step. \n",
      "\n",
      "1. The commissioning phase of the JWST system is mentioned to be completed within six months after launch.\n",
      "2. The requirement states that the planned commissioning phase shall end no later than six months after launch.\n",
      "\n",
      "Since the commissioning phase is planned to be completed within six months after launch, the requirement is satisfied.\n",
      "\n",
      "Final Answer: Yes\n",
      "'''python\n",
      "# No need for any calculations here. The requirement is directly mentioned in the system description.\n",
      "print(\"Final Answer: Yes\")'''python\n",
      "\n",
      "Final Answer: Yes<|end_of_turn|>\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(complete_output.scores)#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete_output.scores#.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "softmax() received an invalid combination of arguments - got (tuple, int), but expected one of:\n * (Tensor input, int dim, torch.dtype dtype, *, Tensor out)\n * (Tensor input, name dim, *, torch.dtype dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scores\u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomplete_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: softmax() received an invalid combination of arguments - got (tuple, int), but expected one of:\n * (Tensor input, int dim, torch.dtype dtype, *, Tensor out)\n * (Tensor input, name dim, *, torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "scores= torch.softmax(complete_output.scores,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "#torch.tensor(complete_output.scores)\n",
    "#torch.softmax(complete_output.scores[-1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_output.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_sequence_uncertainty(output):\n",
    "\n",
    "    scores= []\n",
    "\n",
    "    ## calculate the geometric mean of the sequence (becoming an arithmetic mean log-probability) of the most likely token, ignoring that on generation another token can be chosen than the most likeliest. \n",
    "    for i in range(len(output.scores)):\n",
    "        \n",
    "        ## https://huggingface.co/docs/transformers/v4.36.1/en/internal/generation_utils#transformers.generation.SampleEncoderDecoderOutput\n",
    "        ## gets probabilities from raw prediction scores of the lm head for each token\n",
    "        score = torch.softmax(output.scores[i],1)\n",
    "        ## gets the most likely token score\n",
    "        score = torch.max(score).item()\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    ## https://huggingface.co/docs/transformers/perplexity\n",
    "    log_scores = np.log(np.array(scores))#.shape\n",
    "    mean = -np.sum(log_scores)/len(scores)\n",
    "    return round(mean,2)\n",
    "\n",
    "calculate_sequence_uncertainty(complete_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_scores(output):\n",
    "    # Specify the layers and heads you are interested in\n",
    "    layers = [-1, -8, -16, -24]\n",
    "    #heads = [0, 1, 2, 3, 4, ,5] ### create list for 32 heads\n",
    "    heads = [num for num in range(32)]\n",
    "    #heads =  # Example heads, modify as needed\n",
    "\n",
    "    output_attention_scores = {}\n",
    "    for layer in layers:\n",
    "        for head in heads:\n",
    "            # Get attention score of last generated token [-1] in specific layer [layer] sequence[0] and head [head]\n",
    "            # AND Move to CPU and convert to float32 or float64\n",
    "            \n",
    "            attention_tensor = output.attentions[-1][layer][0][head][-1].to(device='cpu', dtype=torch.float32)\n",
    "\n",
    "            # Convert to a list\n",
    "            tensor_list = attention_tensor.tolist()\n",
    "\n",
    "            # Store in the dictionary\n",
    "            output_attention_scores[f\"layer_{layer}_head_{head}_last_token\"] = tensor_list\n",
    "\n",
    "    return output_attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e63ad40fdc849918daf865b66b78cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/miniconda3/envs/llm_updated/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/paul/miniconda3/envs/llm_updated/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME =\"likenneth/honest_llama2_chat_7B\"#\"openchat/openchat_3.5\"#\"lmsys/vicuna-13b-v1.5\"#\"openchat/openchat_3.5\"#\"lmsys/vicuna-13b-v1.5\" #\"mistralai/Mistral-7B-v0.1\"\n",
    "LOAD_8BIT = True \n",
    "# #LlamaForCausalLM.from_pretrained(MODEL_NAME,\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME,\n",
    "        load_in_8bit=LOAD_8BIT,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME =\"likenneth/honest_llama2_chat_7B\"#\"openchat/openchat_3.5\"#\"lmsys/vicuna-13b-v1.5\"#\"openchat/openchat_3.5\"#\"lmsys/vicuna-13b-v1.5\" #\"mistralai/Mistral-7B-v0.1\"\n",
    "LOAD_8BIT = True \n",
    "# #LlamaForCausalLM.from_pretrained(MODEL_NAME,\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME,\n",
    "        load_in_8bit=LOAD_8BIT,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(0, \"/home/jovyan/work/honest_llama2_chat_7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaAttention(\n",
       "  (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "  (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "  (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "  (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "  (rotary_emb): LlamaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.model.layers[0].mlp\n",
    "model.model.layers[0].self_attn#.head_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baukit import Trace, TraceDict\n",
    "\n",
    "def get_llama_activations_bau(model, prompt, device): \n",
    "\n",
    "    #HEADS = [f\"model.layers.{i}.self_attn.head_out\" for i in range(model.config.num_hidden_layers)]\n",
    "    ## https://github.com/likenneth/honest_llama/issues/7\n",
    "    HEADS = [f\"model.layers.{i}.self_attn.o_proj\" for i in range(model.config.num_hidden_layers)]\n",
    "    MLPS = [f\"model.layers.{i}.mlp\" for i in range(model.config.num_hidden_layers)]\n",
    "    #print(HEADS)\n",
    "    with torch.no_grad():\n",
    "        prompt = prompt.to(device)\n",
    "        with TraceDict(model, HEADS+MLPS, retain_input=True) as ret:\n",
    "           #output = model.generate(prompt, return_dict_in_generate=True, output_hidden_states = True, output_scores=True)\n",
    "            output = model(prompt, output_hidden_states = True)#,return_dict_in_generate=True, output_scores=True)\n",
    "        \n",
    "            #output = output[1]\n",
    "            hidden_states = output.hidden_states\n",
    "            hidden_states = torch.stack(hidden_states, dim = 0).squeeze()\n",
    "            hidden_states = hidden_states.detach().cpu().numpy()\n",
    "            #head_wise_hidden_states = [ret[head].output.squeeze().detach().cpu() for head in HEADS]\n",
    "            # change from ret[head].output  to ret[head].input\n",
    "\n",
    "            ## get attention inputs before merging transformation layer ,\n",
    "            ## https://github.com/likenneth/honest_llama/issues/7\n",
    "            head_wise_hidden_states = [ret[head].input.squeeze().detach().cpu() for head in HEADS]\n",
    "            head_wise_hidden_states = torch.stack(head_wise_hidden_states, dim = 0).squeeze().numpy()\n",
    "            mlp_wise_hidden_states = [ret[mlp].output.squeeze().detach().cpu() for mlp in MLPS]\n",
    "            mlp_wise_hidden_states = torch.stack(mlp_wise_hidden_states, dim = 0).squeeze().numpy()\n",
    "\n",
    "    return hidden_states, head_wise_hidden_states, mlp_wise_hidden_states, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_reason = \"What is the colour of the sky?\"\n",
    "inputs = tokenizer(prompt_reason, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]#.to(device)\n",
    "device = \"cuda\"\n",
    "layer_wise_activations, head_wise_activations, _, output = get_llama_activations_bau(model, input_ids, device)\n",
    "# get_llama_activations_bau(model, input_ids, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 9, 4096)\n",
      "(32, 9, 32, 128)\n"
     ]
    }
   ],
   "source": [
    "num_layers = model.config.num_hidden_layers\n",
    "num_heads = model.config.num_attention_heads\n",
    "sequence_length = len(tokenizer(prompt_reason, return_tensors=\"pt\")['input_ids'][0])\n",
    "dimension_of_heads = model.config.hidden_size // num_heads\n",
    "print(head_wise_activations.shape) ## [Layers, sequence_length, hidden_size]\n",
    "head_wise_activations_reshaped = head_wise_activations.reshape(num_layers, sequence_length, num_heads, dimension_of_heads)\n",
    "print(head_wise_activations_reshaped.shape) ## [Layers, sequence_length, number_of_heads, dimension_of_heads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1179648 into shape (1,9,32,128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Add a batch dimension and then reshape\u001b[39;00m\n\u001b[1;32m     10\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(head_wise_activations, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Adds a batch dimension: shape becomes (1, 32, 9, 4096)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mattn_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Reshape to the desired shape\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(attn_output\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Should be (1, 9, 32, 128)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Step 1: Reshape to separate the heads and head dimensions\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#attn_output = attn_output.reshape(query_length, num_heads, head_dim)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1179648 into shape (1,9,32,128)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_heads = 32\n",
    "head_dim = model.config.hidden_size // num_heads\n",
    "attn_output = head_wise_activations\n",
    "batch_size = 1\n",
    "query_length = len(tokenizer(prompt_reason, return_tensors=\"pt\")['input_ids'][0])\n",
    "\n",
    "# Add a batch dimension and then reshape\n",
    "attn_output = np.expand_dims(head_wise_activations, axis=0)  # Adds a batch dimension: shape becomes (1, 32, 9, 4096)\n",
    "attn_output = attn_output.reshape(batch_size, query_length, num_heads, head_dim)  # Reshape to the desired shape\n",
    "\n",
    "print(attn_output.shape)  # Should be (1, 9, 32, 128)\n",
    "\n",
    "# Step 1: Reshape to separate the heads and head dimensions\n",
    "#attn_output = attn_output.reshape(query_length, num_heads, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36864"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_size = batch_size * query_length * num_heads * head_dim\n",
    "expected_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 9, 4096)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 4096)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_wise_activations[:,-1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralConfig {\n",
       "  \"_name_or_path\": \"openchat/openchat_3.5\",\n",
       "  \"architectures\": [\n",
       "    \"MistralForCausalLM\"\n",
       "  ],\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 32000,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 14336,\n",
       "  \"max_position_embeddings\": 8192,\n",
       "  \"model_type\": \"mistral\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"quantization_config\": {\n",
       "    \"bnb_4bit_compute_dtype\": \"float32\",\n",
       "    \"bnb_4bit_quant_type\": \"fp4\",\n",
       "    \"bnb_4bit_use_double_quant\": false,\n",
       "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "    \"llm_int8_has_fp16_weight\": false,\n",
       "    \"llm_int8_skip_modules\": null,\n",
       "    \"llm_int8_threshold\": 6.0,\n",
       "    \"load_in_4bit\": false,\n",
       "    \"load_in_8bit\": true,\n",
       "    \"quant_method\": \"bitsandbytes\"\n",
       "  },\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"sliding_window\": 4096,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.35.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32002\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32001"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['<|pad_0|>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f46c8cee26450e86cd2c9673034b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/miniconda3/envs/llm_updated/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM#, # AutoModelForCausalLM, AutoTokenizer#, LlamaForCausa\n",
    "import torch \n",
    "\n",
    "MODEL_NAME =\"openchat/openchat_3.5\"#\"likenneth/honest_llama2_chat_7B\"#\"openchat/openchat_3.5\"#\"lmsys/vicuna-13b-v1.5\"#\"openchat/openchat_3.5\"#\"lmsys/vicuna-13b-v1.5\" #\"mistralai/Mistral-7B-v0.1\"\n",
    "LOAD_8BIT = True \n",
    "# #LlamaForCausalLM.from_pretrained(MODEL_NAME,\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME,\n",
    "        load_in_8bit=LOAD_8BIT,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        )\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if \"open\" in model.config._name_or_path:\n",
    "\n",
    "    model.generation_config.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADS = [f\"model.layers.{i}.self_attn.head_out\" for i in range(model.config.num_hidden_layers)]\n",
    "#HEADS = [f\"model.layers.{i}.self_attn.o_proj\" for i in range(model.config.num_hidden_layers)]\n",
    "MLPS = [f\"model.layers.{i}.mlp\" for i in range(model.config.num_hidden_layers)]\n",
    "#print(HEADS)\n",
    "device = \"cuda\"\n",
    "with torch.no_grad():\n",
    "    \n",
    "    prompt = prompt.to(device)\n",
    "    with TraceDict(model, HEADS+MLPS) as ret:\n",
    "        output = model(prompt, output_hidden_states = True)#,return_dict_in_generate=True, output_scores=True)\n",
    "    \n",
    "    #output = output[1]\n",
    "    #hidden_states = output.hidden_states\n",
    "    #hidden_states = torch.stack(hidden_states, dim = 0).squeeze()\n",
    "    #hidden_states = hidden_states.detach().cpu().numpy()\n",
    "    head_wise_hidden_states = [ret[head].output.squeeze().detach().cpu() for head in HEADS]\n",
    "    head_wise_hidden_states = torch.stack(head_wise_hidden_states, dim = 0).squeeze().numpy()\n",
    "    mlp_wise_hidden_states = [ret[mlp].output.squeeze().detach().cpu() for mlp in MLPS]\n",
    "    mlp_wise_hidden_states = torch.stack(mlp_wise_hidden_states, dim = 0).squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(HEADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.model#.layers[0]#.self_attn.o_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#head_wise_hidden_states#.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[head for head in HEADS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 4096])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret[HEADS[0]].output.shape#.squeeze().detach().cpu().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 20, 4096)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_wise_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'head_wise_hidden_states' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m num_heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m      2\u001b[0m head_dim \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_heads\n\u001b[0;32m----> 3\u001b[0m att_output \u001b[38;5;241m=\u001b[39m \u001b[43mhead_wise_hidden_states\u001b[49m\n\u001b[1;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      5\u001b[0m query_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenizer(prompt_1, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'head_wise_hidden_states' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_wise_hidden_states = [ret[mlp].output.squeeze().detach().cpu() for mlp in MLPS]\n",
    "len(mlp_wise_hidden_states)#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 4096])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_wise_hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_wise_hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#prompt_reason = \"What is the colour of the sky?\"\n",
    "#prompt_reason = [\"What is the colour of the sky?\", ]\n",
    "tokenizer.pad_token_id = 32001#.special_token#.config\n",
    "prompt_1 = \"What is the colour of the sky?\"\n",
    "prompt_2 = \"What is the colour of the sky?\\n\\nThe sky is blue.\\n\\nWhy is\"\n",
    "\n",
    "device = \"cuda\"\n",
    "inputs = tokenizer(prompt_reason, return_tensors=\"pt\", padding= True)\n",
    "prompt = inputs[\"input_ids\"].to(device)\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(prompt_1, return_tensors=\"pt\", padding= True)\n",
    "    prompt = inputs[\"input_ids\"].to(device)\n",
    "    output_1 = model.generate(prompt,return_dict_in_generate=True, output_hidden_states = True, temperature = 0, output_scores= True)#, output_scores=True, )\n",
    "    \n",
    "    inputs = tokenizer(prompt_2, return_tensors=\"pt\", padding= True)\n",
    "    prompt = inputs[\"input_ids\"].to(device)\n",
    "    output_2 = model(prompt, output_hidden_states = True)#, output_scores=True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden_states tranformation between 1st and 2nd generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output_1\n",
      "Length prompt 9\n",
      "Number of generated tokens:\n",
      "11\n",
      "11\n",
      "Number of Layers:\n",
      "33\n",
      "Dimension of hidden state of token: 3 and layer: 0\n",
      "torch.Size([1, 1, 4096])\n",
      "Hidden state of token: 3 and layer: 0 of batch number: 0\n",
      "torch.Size([1, 4096])\n",
      "Hidden state of token: 3 and layer: 0 of batch number: 0 and processed token: 3\n",
      "tensor([ 0.0014, -0.0018, -0.0018,  ...,  0.0014,  0.0017, -0.0006],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "_________________\n",
      "Output_2\n",
      "Current index for hidden_state:  11\n",
      "Number of Layers:\n",
      "33\n",
      "Dimension of hidden state of layer: 0\n",
      "torch.Size([1, 20, 4096])\n",
      "Hidden state of token: 11 and layer: 0 of batch number: 0\n",
      "torch.Size([20, 4096])\n",
      "tensor([ 0.0014, -0.0018, -0.0018,  ...,  0.0014,  0.0017, -0.0006],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#prompt_reason = \"What is the colour of the sky?\"\n",
    "#prompt_reason = [\"What is the colour of the sky?\", ]\n",
    "tokenizer.pad_token_id = 32001#.special_token#.config\n",
    "prompt_1 = \"What is the colour of the sky?\"\n",
    "prompt_2 = \"What is the colour of the sky?\\n\\nThe sky is blue.\\n\\nWhy is\"\n",
    "\n",
    "device = \"cuda\"\n",
    "inputs = tokenizer(prompt_reason, return_tensors=\"pt\", padding= True)\n",
    "prompt = inputs[\"input_ids\"].to(device)\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(prompt_1, return_tensors=\"pt\", padding= True)\n",
    "    prompt = inputs[\"input_ids\"].to(device)\n",
    "    output_1 = model.generate(prompt,return_dict_in_generate=True, output_hidden_states = True, temperature = 0, output_scores= True)#, output_scores=True, )\n",
    "    \n",
    "    inputs = tokenizer(prompt_2, return_tensors=\"pt\", padding= True)\n",
    "    prompt = inputs[\"input_ids\"].to(device)\n",
    "    output_2 = model(prompt, output_hidden_states = True)#, output_scores=True\n",
    "\n",
    "print(\"Output_1\")\n",
    "prompt_length = len(tokenizer(prompt_1, return_tensors=\"pt\", padding= True)['input_ids'][0])\n",
    "print(\"Length prompt\",prompt_length)\n",
    "print(\"Number of generated tokens:\")\n",
    "print(len(output_1.hidden_states))\n",
    "print(len(output_1.scores))\n",
    "## first token both hidden states \n",
    "layer = 0 \n",
    "token = 3\n",
    "## hidden_state = [num_tokens,layer, batch_size, number_of_processed_tokens, dimension]\n",
    "number_of_processed_tokens = 0\n",
    "sequence = 0\n",
    "print(\"Number of Layers:\")\n",
    "print(len(output_1.hidden_states[token]))\n",
    "print(f\"Dimension of hidden state of token: {token} and layer: {layer}\")\n",
    "print(output_1.hidden_states[token][layer].shape)\n",
    "print(f\"Hidden state of token: {token} and layer: {layer} of batch number: 0\")\n",
    "print(output_1.hidden_states[token][layer][sequence].shape)\n",
    "print(f\"Hidden state of token: {token} and layer: {layer} of batch number: {sequence} and processed token: {token}\")\n",
    "print(output_1.hidden_states[token][layer][sequence][number_of_processed_tokens])\n",
    "\n",
    "print(\"_________________\")\n",
    "print(\"Output_2\")\n",
    "number_of_processed_tokens = prompt_length+token-1 if token != 0  else number_of_processed_tokens#number_of_processed_tokens + token\n",
    "print(\"Current index for hidden_state: \", number_of_processed_tokens)\n",
    "print(\"Number of Layers:\")\n",
    "print(len(output_2.hidden_states))\n",
    "\n",
    "print(f\"Dimension of hidden state of layer: {layer}\")\n",
    "print(output_2.hidden_states[layer].shape)\n",
    "print(f\"Hidden state of token: {number_of_processed_tokens} and layer: {layer} of batch number: {sequence}\")\n",
    "print(output_2.hidden_states[layer][sequence].shape)#.shape)\n",
    "print(output_2.hidden_states[layer][sequence][number_of_processed_tokens])#.shape)\n",
    "\n",
    "\n",
    "#### Is the encoding by the model.generate as model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.2684e-04,  2.9945e-04, -4.6730e-05,  ..., -5.3644e-07,\n",
      "         1.7643e-04, -1.3173e-05], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(output_1.hidden_states[0][token][layer][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(len(output_1.hidden_states))#[0][token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0045,  0.0003, -0.0052,  ..., -0.0004, -0.0012,  0.0002],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0045,  0.0003, -0.0052,  ..., -0.0004, -0.0012,  0.0002],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_reason = \"What is the colour of the sky?\"\n",
    "\n",
    "inputs = tokenizer(prompt_reason, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]#.to(device)\n",
    "device = \"cuda\"\n",
    "layer_wise_activations, head_wise_activations, _, output = get_llama_activations_bau(model, input_ids, device)\n",
    "# get_llama_activations_bau(model, input_ids, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output.hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 9, 4096)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_wise_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9])\n",
      "33\n",
      "9\n",
      "33\n",
      "torch.Size([9, 4096])\n",
      "torch.Size([9, 4096])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(input_ids.shape)\n",
    "## Length number of generated tokens \n",
    "#print(len(output.scores))\n",
    "## Length of hidden states \n",
    "print(len(output.hidden_states))#[-1][-1].shape\n",
    "layer_wise_activations.shape\n",
    "complete_output = output\n",
    "## Length tokenized input\n",
    "print(len(tokenizer.encode(prompt_reason)))\n",
    "## Length number of generated tokens \n",
    "#print(len(complete_output.scores))\n",
    "## Length of hidden states \n",
    "print(len(complete_output.hidden_states))#[-1][-1].shape\n",
    "## Dimension of hidden state of last generated token and last layer\n",
    "print(complete_output.hidden_states[-1][-1].shape)#[-1][-1].shape\n",
    "## Dimension of hidden state of first generated token and last layer\n",
    "print(complete_output.hidden_states[0][-1].shape)#[-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9])\n",
      "11\n",
      "11\n",
      "9\n",
      "11\n",
      "11\n",
      "torch.Size([1, 1, 4096])\n",
      "torch.Size([1, 9, 4096])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(input_ids.shape)\n",
    "## Length number of generated tokens \n",
    "print(len(output.scores))\n",
    "## Length of hidden states \n",
    "print(len(output.hidden_states))#[-1][-1].shape\n",
    "layer_wise_activations.shape\n",
    "complete_output = output\n",
    "## Length tokenized input\n",
    "print(len(tokenizer.encode(prompt_reason)))\n",
    "## Length number of generated tokens \n",
    "print(len(complete_output.scores))\n",
    "## Length of hidden states \n",
    "print(len(complete_output.hidden_states))#[-1][-1].shape\n",
    "## Dimension of hidden state of last generated token and last layer\n",
    "print(complete_output.hidden_states[-1][-1].shape)#[-1][-1].shape\n",
    "## Dimension of hidden state of first generated token and last layer\n",
    "print(complete_output.hidden_states[0][-1].shape)#[-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(output.hidden_states[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 4096])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.hidden_states[0][]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output.loss#.['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model.layers[0].self_attn.head_out   #.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "465"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(len(attentions['layer_-1_head_0_last_token']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "465\n"
     ]
    }
   ],
   "source": [
    "print(len(attentions['layer_-16_head_30_last_token']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#output, complete_output = evaluate(prompt_reason, model = model, tokenizer= tokenizer, stopping_criteria = None, device = \"cuda\", **extra_args) #\u001b[39;00m\n\u001b[1;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m layer_wise_activations, head_wise_activations, _ \u001b[38;5;241m=\u001b[39m \u001b[43mget_llama_activations_bau\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_reason\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 7\u001b[0m, in \u001b[0;36mget_llama_activations_bau\u001b[0;34m(model, prompt, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m MLPS \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.layers.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mlp\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m TraceDict(model, HEADS\u001b[38;5;241m+\u001b[39mMLPS) \u001b[38;5;28;01mas\u001b[39;00m ret:\n\u001b[1;32m      9\u001b[0m         output \u001b[38;5;241m=\u001b[39m model(prompt, output_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#output, complete_output = evaluate(prompt_reason, model = model, tokenizer= tokenizer, stopping_criteria = None, device = \"cuda\", **extra_args) #\n",
    "device = \"cuda\"\n",
    "layer_wise_activations, head_wise_activations, _ = get_llama_activations_bau(model, prompt_reason, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352\n",
      "114\n",
      "114\n",
      "torch.Size([1, 32, 1, 465])\n",
      "torch.Size([1, 32, 352, 352])\n"
     ]
    }
   ],
   "source": [
    "## Length tokenized input\n",
    "print(len(tokenizer.encode(prompt_reason)))\n",
    "## Length number of generated tokens \n",
    "print(len(complete_output.scores))\n",
    "## Length of hidden states \n",
    "print(len(complete_output.attentions))#[-1][-1].shape\n",
    "## Dimension of hidden state of last generated token and last layer\n",
    "print(complete_output.attentions[-1][-1].shape)#[-1][-1].shape\n",
    "## Dimension of hidden state of first generated token and last layer\n",
    "print(complete_output.attentions[0][-1].shape)#[-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "600\n",
      "600\n",
      "torch.Size([1, 32, 1, 608])\n",
      "torch.Size([1, 32, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "## Length tokenized input\n",
    "print(len(tokenizer.encode(prompt_reason)))\n",
    "## Length number of generated tokens \n",
    "print(len(complete_output.scores))\n",
    "## Length of hidden states \n",
    "print(len(complete_output.attentions))#[-1][-1].shape\n",
    "## Dimension of hidden state of last generated token and last layer\n",
    "print(complete_output.attentions[-1][-1].shape)#[-1][-1].shape\n",
    "## Dimension of hidden state of first generated token and last layer\n",
    "print(complete_output.attentions[0][-1].shape)#[-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 608])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_output.attentions[-1][-1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 1, 608])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_output.attentions[-1][-1].shape#[0][0][0]#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Length tokenized input\n",
    "print(len(tokenizer.encode(prompt_reason)))\n",
    "## Length number of generated tokens \n",
    "print(len(complete_output.scores))\n",
    "## Length of hidden states \n",
    "print(len(complete_output.attentions))#[-1][-1].shape\n",
    "## Dimension of hidden state of last generated token and last layer\n",
    "print(complete_output.attentions[-1][-1].shape)#[-1][-1].shape\n",
    "## Dimension of attentions of first generated token and last layer\n",
    "print(complete_output.attentions[0][-1].shape)#[-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "print(complete_output.attentions[0][-1].shape)#[-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-7.2734, -6.3906,  9.2891,  ..., -2.6016, 13.9531, -2.3145]],\n",
       "        device='cuda:0'),\n",
       " 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_output.scores[-1],1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(complete_output.sequences[0])#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> GPT4 Correct User: System: |JWST| |isPlacedIn| |Transfer Orbit| by |Launch Vehicle|\\n|JWST| |orbits| |Second LaGrange Point (L2)|\\n|JWST| |maintains_ZExcursion| \"≤500,000 Km\"\\n|JWST| |maintains_YExcursion| \"≤800,000 Km\"\\n|JWST| |avoids| |Eclipse|\\n|JWST| |transfersTo| |L2 Operational Orbit| using |DUA IRD (JWST-IRD-003674)|\\n|JWST| |has_MissionLifetime| \"5 years\"\\n|JWST| |completes_Commissioning| within \"6 months\"\\n|JWST System| |delivers_RealTimeTelemetry| \"92.5%\"\\n|JWST System| |archives_RecordedData| \"97%\"\\n|JWST| |achieves_Sensitivity| per |NIRCam, NIRSpec, MIRI, FGS-TF Calculations|\\nRequirement: The planned commissioning phase shall end no later than six months after launch.\\nBased on the system description, is this requirement satisfied? Include in your answer \"Final Answer: Yes/No\". Use Python scripting if you need to calculate something by starting with \\'\\'\\'python... Let\\'s think step-by-step. \\n\\n1. The commissioning phase is mentioned as \"completes_Commissioning within \"6 months\"\". \\n2. The requirement states \"The planned commissioning phase shall end no later than six months after launch\". \\n\\nSince the commissioning phase is planned to be completed within 6 months, the requirement is satisfied. \\n\\nFinal Answer: Yes'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(complete_output.sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> GPT4 Correct User: System: |JWST| |isPlacedIn| |Transfer Orbit| by |Launch Vehicle|\n",
      "|JWST| |orbits| |Second LaGrange Point (L2)|\n",
      "|JWST| |maintains_ZExcursion| \"≤500,000 Km\"\n",
      "|JWST| |maintains_YExcursion| \"≤800,000 Km\"\n",
      "|JWST| |avoids| |Eclipse|\n",
      "|JWST| |transfersTo| |L2 Operational Orbit| using |DUA IRD (JWST-IRD-003674)|\n",
      "|JWST| |has_MissionLifetime| \"5 years\"\n",
      "|JWST| |completes_Commissioning| within \"6 months\"\n",
      "|JWST System| |delivers_RealTimeTelemetry| \"92.5%\"\n",
      "|JWST System| |archives_RecordedData| \"97%\"\n",
      "|JWST| |achieves_Sensitivity| per |NIRCam, NIRSpec, MIRI, FGS-TF Calculations|\n",
      "Requirement: The planned commissioning phase shall end no later than six months after launch.\n",
      "Based on the system description, is this requirement satisfied? Include in your answer \"Final Answer: Yes/No\". Use Python scripting if you need to calculate something by starting with '''python... Let's think step-by-step. \n",
      "\n",
      "1. The commissioning phase is mentioned as \"completes_Commissioning within \"6 months\"\". \n",
      "2. The requirement states \"The planned commissioning phase shall end no later than six months after launch\". \n",
      "\n",
      "Since the commissioning phase is planned to be completed within 6 months, the requirement is satisfied. \n",
      "\n",
      "Final Answer: Yes\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(complete_output.hidden_states[-1][-1].shape)#[-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(complete_output.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.3008, -7.4531,  1.6777,  ..., -3.0215,  5.3516, -2.2734]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_output.scores[0]#)#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32002])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_output.scores[0].shape#)#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.3635e-09, device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "torch.exp(torch.max(complete_output.scores[0]))# .shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352\n",
      "114\n",
      "114\n",
      "torch.Size([1, 32, 1, 465])\n",
      "torch.Size([1, 32, 352, 352])\n"
     ]
    }
   ],
   "source": [
    "## Length tokenized input\n",
    "print(len(tokenizer.encode(prompt_reason)))\n",
    "## Length number of generated tokens \n",
    "print(len(complete_output.scores))\n",
    "## Length of hidden states \n",
    "print(len(complete_output.attentions))#[-1][-1].shape\n",
    "## Dimension of hidden state of last generated token and last layer\n",
    "print(complete_output.attentions[-1][-1].shape)#[-1][-1].shape\n",
    "## Dimension of hidden state of first generated token and last layer\n",
    "print(complete_output.attentions[0][-1].shape)#[-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352\n",
      "114\n",
      "114\n",
      "torch.Size([1, 32, 1, 465])\n",
      "torch.Size([1, 32, 352, 352])\n"
     ]
    }
   ],
   "source": [
    "layer = -8\n",
    "head = 8\n",
    "\n",
    "## Length tokenized input\n",
    "print(len(tokenizer.encode(prompt_reason)))\n",
    "## Length number of generated tokens \n",
    "print(len(complete_output.scores))\n",
    "## Length of hidden states \n",
    "print(len(complete_output.attentions))#[-1][-1].shape\n",
    "## Dimension of hidden state of last generated token and [layer] layer\n",
    "print(complete_output.attentions[-1][layer][head].shape)#[-1][-1].shape\n",
    "## Dimension of hidden state of first generated token and [layer] layer\n",
    "print(complete_output.attentions[0][layer].shape)#[-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([465])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = -8\n",
    "head = 8\n",
    "sequence = -1\n",
    "complete_output.attentions[-1][layer][sequence][head][-1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[num for num in range(32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compelte_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompelte_output\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compelte_output' is not defined"
     ]
    }
   ],
   "source": [
    "complete_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1860.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "465*32/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352\n",
      "114\n",
      "114\n",
      "torch.Size([1, 1, 4096])\n",
      "torch.Size([1, 352, 4096])\n"
     ]
    }
   ],
   "source": [
    "## Length tokenized input\n",
    "print(len(tokenizer.encode(prompt_reason)))\n",
    "## Length number of generated tokens \n",
    "print(len(complete_output.scores))\n",
    "## Length of hidden states \n",
    "print(len(complete_output.hidden_states))#[-1][-1].shape\n",
    "## Dimension of hidden state of last generated token and last layer\n",
    "print(complete_output.hidden_states[-1][-1].shape)#[-1][-1].shape\n",
    "## Dimension of hidden state of first generated token and last layer\n",
    "print(complete_output.hidden_states[0][-1].shape)#[-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352\n",
      "114\n",
      "114\n",
      "torch.Size([1, 1, 4096])\n",
      "torch.Size([1, 352, 4096])\n"
     ]
    }
   ],
   "source": [
    "## Length tokenized input\n",
    "print(len(tokenizer.encode(prompt_reason)))\n",
    "## Length number of generated tokens \n",
    "print(len(complete_output.scores))\n",
    "## Length of hidden states \n",
    "print(len(complete_output.hidden_states))#[-1][-1].shape\n",
    "## Dimension of hidden state of last generated token and last layer\n",
    "print(complete_output.hidden_states[-1][-1].shape)#[-1][-1].shape\n",
    "## Dimension of hidden state of first generated token and last layer\n",
    "print(complete_output.hidden_states[0][-1].shape)#[-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = -1\n",
    "complete_output.hidden_states[-1][layer][-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.0195,  2.8145, -0.4246,  ..., -2.5488,  6.7031,  1.4111],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_output.hidden_states[0][layer][-1][-1]#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_output.hidden_states[0][layer][0][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.0195,  2.8145, -0.4246,  ..., -2.5488,  6.7031,  1.4111],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_output.hidden_states[0][layer][-1][-1]#.shape#[-1]#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_output.hidden_states[8][layer][-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4096])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer.decode(complete_output.sequences[0][8])\n",
    "complete_output.hidden_states[-1][-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.0195,  2.8145, -0.4246,  ..., -2.5488,  6.7031,  1.4111]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer.decode(complete_output.sequences[0][8])\n",
    "complete_output.hidden_states[0][-1][-1][-1:]#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.0195,  2.8145, -0.4246,  ..., -2.5488,  6.7031,  1.4111],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_output.hidden_states[0][-1][-1][-1]#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(complete_output.hidden_states[0][8][-1][-1:].shape)#[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(complete_output.hidden_states[0][-1][-1][-1:].shape)#[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.0195,  2.8145, -0.4246,  ..., -2.5488,  6.7031,  1.4111]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_output.hidden_states[0][-1][-1][-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.0195,  2.8145, -0.4246,  ..., -2.5488,  6.7031,  1.4111],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_output.hidden_states[0][-1][-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(complete_output.hidden_states)#[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> GPT4 Correct User: System: |JWST| |isPlacedIn| |Transfer Orbit| by |Launch Vehicle|\\n|JWST| |orbits| |Second LaGrange Point (L2)|\\n|JWST| |maintains_ZExcursion| \"≤500,000 Km\"\\n|JWST| |maintains_YExcursion| \"≤800,000 Km\"\\n|JWST| |avoids| |Eclipse|\\n|JWST| |transfersTo| |L2 Operational Orbit| using |DUA IRD (JWST-IRD-003674)|\\n|JWST| |has_MissionLifetime| \"5 years\"\\n|JWST| |completes_Commissioning| within \"6 months\"\\n|JWST System| |delivers_RealTimeTelemetry| \"92.5%\"\\n|JWST System| |archives_RecordedData| \"97%\"\\n|JWST| |achieves_Sensitivity| per |NIRCam, NIRSpec, MIRI, FGS-TF Calculations|\\nRequirement: The planned commissioning phase shall end no later than six months after launch.\\nBased on the system description, is this requirement satisfied? Include in your answer \"Final Answer: Yes/No\". Use Python scripting if you need to calculate something by starting with \\'\\'\\'python... Let\\'s think step-by-step. \\n\\n1. The commissioning phase of the JWST system is mentioned to be completed within six months after launch.\\n2. The requirement states that the planned commissioning phase shall end no later than six months after launch.\\n\\nSince the commissioning phase is planned to be completed within six months after launch, the requirement is satisfied.\\n\\nFinal Answer: Yes'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(complete_output.hidden_states[-1][-1][-1].shape)#[-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Please answer the following question. Are there 50 states in the US. Answer (Yes or No ONLY): Yes\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "896"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(prompt_reason))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896\n",
      "1\n",
      "1\n",
      "torch.Size([1, 896, 4096])\n"
     ]
    }
   ],
   "source": [
    "## Length tokenized input\n",
    "print(len(tokenizer.encode(prompt_reason)))\n",
    "## Length number of generated tokens \n",
    "print(len(complete_output.scores))\n",
    "## Length of hidden states \n",
    "print(len(complete_output.hidden_states))#[-1][-1].shape\n",
    "## Dimension of hidden state of last generated token and last layer\n",
    "print(complete_output.hidden_states[-1][-1].shape)#[-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "600\n",
      "600\n",
      "torch.Size([1, 1, 4096])\n"
     ]
    }
   ],
   "source": [
    "## Length tokenized input\n",
    "print(len(tokenizer.encode(prompt_reason)))\n",
    "## Length number of generated tokens \n",
    "print(len(complete_output.scores))\n",
    "## Length of hidden states \n",
    "print(len(complete_output.hidden_states))#[-1][-1].shape\n",
    "## Dimension of hidden state of last generated token and last layer\n",
    "print(complete_output.hidden_states[-1][-1].shape)#[-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "600\n",
      "600\n",
      "torch.Size([1, 1, 4096])\n",
      "torch.Size([1, 9, 4096])\n"
     ]
    }
   ],
   "source": [
    "## Length tokenized input\n",
    "print(len(tokenizer.encode(prompt_reason)))\n",
    "## Length number of generated tokens \n",
    "print(len(complete_output.scores))\n",
    "## Length of hidden states \n",
    "print(len(complete_output.hidden_states))#[-1][-1].shape\n",
    "## Dimension of hidden state of last generated token and last layer\n",
    "print(complete_output.hidden_states[-1][-1].shape)#[-1][-1].shape\n",
    "## Dimension of hidden state of first generated token and last layer\n",
    "print(complete_output.hidden_states[0][-1].shape)#[-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "600\n",
      "600\n",
      "torch.Size([1, 1, 4096])\n",
      "torch.Size([1, 9, 4096])\n"
     ]
    }
   ],
   "source": [
    "## Length tokenized input\n",
    "print(len(tokenizer.encode(prompt_reason)))\n",
    "## Length number of generated tokens \n",
    "print(len(complete_output.scores))\n",
    "## Length of hidden states \n",
    "print(len(complete_output.hidden_states))#[-1][-1].shape\n",
    "## Dimension of hidden state of last generated token and last layer\n",
    "print(complete_output.hidden_states[-1][-1].shape)#[-1][-1].shape\n",
    "## Dimension of hidden state of first generated token and last layer\n",
    "print(complete_output.hidden_states[0][-1].shape)#[-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "1\n",
      "1\n",
      "torch.Size([1, 25, 4096])\n"
     ]
    }
   ],
   "source": [
    "## Length tokenized input\n",
    "print(len(tokenizer.encode(prompt_reason)))\n",
    "## Length number of generated tokens \n",
    "print(len(complete_output.scores))\n",
    "## Length of hidden states \n",
    "print(len(complete_output.hidden_states))#[-1][-1].shape\n",
    "## Dimension of hidden state of last generated token and last layer\n",
    "print(complete_output.hidden_states[-1][-1].shape)#[-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "13\n",
      "13\n",
      "torch.Size([1, 1, 4096])\n"
     ]
    }
   ],
   "source": [
    "## Length tokenized input\n",
    "print(len(tokenizer.encode(prompt_reason)))\n",
    "## Length number of generated tokens \n",
    "print(len(complete_output.scores))\n",
    "## Length of hidden states \n",
    "print(len(complete_output.hidden_states))#[-1][-1].shape\n",
    "## Dimension of hidden state of last generated token and last layer\n",
    "print(complete_output.hidden_states[-1][-1].shape)#[-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856\n",
      "134\n",
      "134\n",
      "torch.Size([1, 1, 4096])\n"
     ]
    }
   ],
   "source": [
    "## Length tokenized input\n",
    "print(len(tokenizer.encode(prompt_reason)))\n",
    "## Length number of generated tokens \n",
    "print(len(complete_output.scores))\n",
    "## Length of hidden states \n",
    "print(len(complete_output.hidden_states))#[-1][-1].shape\n",
    "## Dimension of hidden state of last generated token and last layer\n",
    "print(complete_output.hidden_states[-1][-1].shape)#[-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856\n",
      "73\n",
      "73\n",
      "torch.Size([1, 1, 4096])\n"
     ]
    }
   ],
   "source": [
    "## Length tokenized input\n",
    "print(len(tokenizer.encode(prompt_reason)))\n",
    "## Length number of generated tokens \n",
    "print(len(complete_output.scores))\n",
    "## Length of hidden states \n",
    "print(len(complete_output.hidden_states))#[-1][-1].shape\n",
    "## Dimension of hidden state of last generated token and last layer\n",
    "print(complete_output.hidden_states[-1][-1].shape)#[-1][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "## hidden state of last layer ([-1]) is a tensor of shape (batch_size, sequence_length, hidden_size)\n",
    "print(complete_output.hidden_states[0][-1].shape)#[].shape#\n",
    "## get hidden state of \"yes\" token in last layer ([-1]) \n",
    "print(complete_output.hidden_states[0][-1][0][-1].shape)#[].shape#\n",
    "#https://huggingface.co/docs/transformers/v4.35.2/en/main_classes/output#transformers.utils.ModelOutput\n",
    "#https://github.com/huggingface/transformers/issues/26174\n",
    "import torch \n",
    "import json \n",
    "\n",
    "layers = [-1, -8, -16, -24]\n",
    "\n",
    "output_hidden_states = {}\n",
    "for layer in layers:\n",
    "\n",
    "    ## get hidden state of last token in specific layer ([-1]) \n",
    "    tensor = complete_output.hidden_states[-1][layer][0][-1]\n",
    "    print(complete_output.hidden_states[-1][layer][0][-1])\n",
    "    # Move to CPU and convert to float32 or float64\n",
    "    tensor =  tensor.to(device='cpu', dtype=torch.float32)\n",
    "    # Convert to a list\n",
    "    tensor_list = tensor.tolist()\n",
    "    # Serialize to JSON\n",
    "    #json_output = json.dumps(tensor_list)\n",
    "\n",
    "    output_hidden_states[layer] = tensor_list\n",
    "\n",
    "#output_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 856, 4096])\n",
      "torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "## hidden state of last layer ([-1]) is a tensor of shape (batch_size, sequence_length, hidden_size)\n",
    "print(complete_output.hidden_states[0][-1].shape)#[].shape#\n",
    "## get hidden state of \"yes\" token in last layer ([-1]) \n",
    "print(complete_output.hidden_states[0][-1][0][-1].shape)#[].shape#\n",
    "#https://huggingface.co/docs/transformers/v4.35.2/en/main_classes/output#transformers.utils.ModelOutput\n",
    "#https://github.com/huggingface/transformers/issues/26174\n",
    "import torch \n",
    "import json \n",
    "\n",
    "layers = [-1, -8, -16, -24]\n",
    "\n",
    "output_hidden_states = {}\n",
    "for layer in layers:\n",
    "\n",
    "    ## get hidden state of last token in specific layer ([-1]) \n",
    "    tensor = complete_output.hidden_states[0][layer][0][-1]\n",
    "    # Move to CPU and convert to float32 or float64\n",
    "    tensor =  tensor.to(device='cpu', dtype=torch.float32)\n",
    "    # Convert to a list\n",
    "    tensor_list = tensor.tolist()\n",
    "    # Serialize to JSON\n",
    "    #json_output = json.dumps(tensor_list)\n",
    "\n",
    "    output_hidden_states[layer] = tensor_list\n",
    "\n",
    "#output_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.model.layers[0]#[-1]#[-1]#ModuleList#[-1]#.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import json\n",
    "# Optionally, save to a file\n",
    "with open('tensor_output_test.json', 'w') as f:\n",
    "    f.write(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5613, device='cuda:0')\n",
      "Yes\n",
      "0.4222647249698639\n"
     ]
    }
   ],
   "source": [
    "#print(output[0])\n",
    "\n",
    "print(torch.argmax(complete_output.scores[0]))\n",
    "print(tokenizer.decode(torch.argmax(complete_output.scores[0])))\n",
    "scores = torch.softmax(complete_output.scores[-1],1)\n",
    "print(torch.max(scores).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n",
      "Y\n",
      "▁Yes\n",
      "<0x0A>\n",
      "No\n"
     ]
    }
   ],
   "source": [
    "## inverse tokenizer dict\n",
    "inv_vocab = {v:k for k, v in tokenizer.vocab.items()}\n",
    "print(inv_vocab[5613])\n",
    "print(inv_vocab[28802])\n",
    "print(inv_vocab[5592])\n",
    "print(inv_vocab[13])\n",
    "print(inv_vocab[2501])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN: I want you to act as an expert Spacecraft Engineer. I will provide an extract of an Engineering System in triple format as well as a requirement, and it will be your job to analyze if the requirement is satisfied by the engineering system or not. \n",
      "First try to extract the information (entities and relations) from the System that is potentially useful in your analysis. \n",
      "Next try to establish if that information either implicitly or explicitly fulfills the requirement. \n",
      "This could involve performing a gap analysis between the engineering system and the requirement. \n",
      "The Final step is to clearly state if the requirement is satisfied or not by stating FINAL ANSWER: YES or FINAL ANSWER: NO.\n",
      "My first engineering system and requirement are: \n",
      "\n",
      "System = \"|Data-Handling Subsystem| |directlyContainsEntity| |Service Module|\n",
      "|Payload Module| |directlyContainsEntity| |Spacecraft|\n",
      "|Payload Module| |directlyContainsEntity| |Brackets_Misc_PLM|\n",
      "|Payload Module| |directlyContainsEntity| |Instruments Subsystem|\n",
      "|Payload Module| |directlyContainsEntity| |Decon HTR|\n",
      "|Payload Module| |directlyContainsEntity| |Mechanisms Subsystem|\n",
      "|Payload Module| |directlyContainsEntity| |M2 Pointing Mechanism|\n",
      "|Payload Module| |directlyContainsEntity| |Baffle|\"\n",
      "\n",
      "Requirement = \"The DHS shall communicate, command and control the payload instruments through the ICU.\"\n",
      "ASSISTANT: Let's analyze the given System and requirement step-by-step:\n",
      "Step 1:\n",
      "The given engineering system describes the components and their relationships in the spacecraft, including the Data-Handling Subsystem (DHS), Payload Module, Service Module, and various subsystems and instruments contained within the Payload Module.\n",
      "\n",
      "Step 2:\n",
      "The requirement specifies that the Data-Handling Subsystem (DHS) must be able to communicate, command, and control the payload instruments through the ICU (Instrument Control Unit).\n",
      "\n",
      "Step 3:\n",
      "From the given engineering system, it is not explicitly clear if the DHS is capable of communicating, commanding, and controlling the payload instruments through the ICU. There is no mention of the ICU in the system and also does not mention any direct connection between the DHS and the ICU. Therefore, there is a gap in the system that needs to be addressed to meet the given requirement. It is not clear how the DHS communicates with the ICU to command and control the payload instruments.\n",
      "\n",
      "FINAL ANSWER: NO. The provided engineering system does not satisfy the given requirement.\n",
      "\n",
      "HUMAN: My second engineering system and requirement are: \n",
      "\n",
      "\n",
      " System:|JWST| |isPlacedIn| |Transfer Orbit| by |Launch Vehicle|\n",
      "    |JWST| |orbits| |Second LaGrange Point (L2)|\n",
      "    |JWST| |maintains_ZExcursion| \"≤500,000 Km\"|\n",
      "    |JWST| |maintains_YExcursion| \"≤800,000 Km\"|\n",
      "    |JWST| |avoids| |Eclipse|\n",
      "    |JWST| |transfersTo| |L2 Operational Orbit| using |DUA IRD (JWST-IRD-003674)|\n",
      "    |JWST| |has_MissionLifetime| \"5 years\"|\n",
      "    |JWST| |completes_Commissioning| within \"6 months\"|\n",
      "    |JWST System| |delivers_RealTimeTelemetry| \"92.5%\"|\n",
      "    |JWST System| |archives_RecordedData| \"97%\"|\n",
      "    |JWST| |achieves_Sensitivity| per |NIRCam, NIRSpec, MIRI, FGS-TF Calculations|\n",
      "     \n",
      "Requirement: The operational JWST System shall maintain the excursion of the orbit about L2 in the Z direction (defined in Figure 3-2) to less than or equal to 500,000 Km. \n",
      "ASSISTANT: Let's analyze the given System and requirement step-by-step:\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "from test_reasoning import evaluate, generate_cpu_gpu\n",
    "from transformers import GenerationConfig\n",
    "import os \n",
    "\n",
    "MAX_NEW_TOKENS = 512\n",
    "extra_args = {  \"max_new_tokens\":MAX_NEW_TOKENS,\n",
    "                \"min_new_tokens\": 256, \n",
    "                \"do_sample\":True, \n",
    "                \"num_beams\" : 1, \n",
    "                \"num_return_sequences\" : 1, \n",
    "                \"temperature\": 0.8, \n",
    "                \"top_p\": 0.95,\n",
    "                \"begin_suppress_tokens\": [2], \n",
    "                }\n",
    "\n",
    "req= \"The Observatory shall orbit the Second LaGrange Point (L2) of the Sun–Earth system.\"\n",
    "req = \"The operational JWST System shall maintain the excursion of the orbit about L2 in the Z direction (defined in Figure 3-2) to less than or equal to 500,000 Km.\"\n",
    "definition = \"\"\"|JWST| |isPlacedIn| |Transfer Orbit| by |Launch Vehicle|\n",
    "    |JWST| |orbits| |Second LaGrange Point (L2)|\n",
    "    |JWST| |maintains_ZExcursion| \"≤500,000 Km\"|\n",
    "    |JWST| |maintains_YExcursion| \"≤800,000 Km\"|\n",
    "    |JWST| |avoids| |Eclipse|\n",
    "    |JWST| |transfersTo| |L2 Operational Orbit| using |DUA IRD (JWST-IRD-003674)|\n",
    "    |JWST| |has_MissionLifetime| \"5 years\"|\n",
    "    |JWST| |completes_Commissioning| within \"6 months\"|\n",
    "    |JWST System| |delivers_RealTimeTelemetry| \"92.5%\"|\n",
    "    |JWST System| |archives_RecordedData| \"97%\"|\n",
    "    |JWST| |achieves_Sensitivity| per |NIRCam, NIRSpec, MIRI, FGS-TF Calculations|\n",
    "    \"\"\" \n",
    "\n",
    "prompt_reason = \"\"\"Based on the system description, is this requirement satisfied? Include in your answer \"Final Answer: Yes/No\" . Use Python scripting if you need to calculate something by starting with '''python... Let's think step-by-step: \"\"\"\n",
    "system_prompt = \"ASSISTANT:\"#\"GPT4 Correct Assistant: \"\n",
    "user_prompt = \"HUMAN:\"#\"GPT4 Correct User:\"\n",
    "\n",
    "prompt_reason = f\"{user_prompt}System: {definition}\\nRequirement: {req}\\n{prompt_reason}\\n{system_prompt}\" \n",
    "\n",
    "script_dir = \"\"\n",
    "\n",
    "prompt_path = os.path.join(script_dir, './Machine_Learning/prompt_reason.txt')\n",
    "with open(prompt_path, 'r') as f:\n",
    "        prompt_reason = f.read()\n",
    "\n",
    "prompt_reason = f\"{prompt_reason}\\n System:{definition} \\nRequirement: {req} \\nASSISTANT: Let's analyze the given System and requirement step-by-step:\"\n",
    "\n",
    "print(prompt_reason)             \n",
    "#generate_cpu_gpu(inputs, script_dir, generation_config)\n",
    "output = evaluate(prompt_reason, model = model, tokenizer= tokenizer, device = \"cuda\", **extra_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Set the random seed\n",
    "random.seed(22)  # Replace 42 with your desired seed\n",
    "# Make a copy of the list\n",
    "rand_results = results.copy()\n",
    "# Shuffle the copy\n",
    "random.shuffle(rand_results)\n",
    "\n",
    "definition = \"\"\n",
    "for triples in rand_results: \n",
    "\n",
    "    definition += \"|\" + triples[3] +\"|\" +\" \" + \"|\" + triples[4] +\"|\" +\" \" +\"|\"+ triples[5] +\"|\" + \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "System:\n",
      "- |JWST| |isPlacedIn| |Transfer Orbit| by |Launch Vehicle|\n",
      "\n",
      "Requirement:\n",
      "- The Launch Vehicle shall place the Observatory on a trajectory from which the Observatory can transfer itself to its operational orbit.\n",
      "\n",
      "From the given system information, we can see that the Launch Vehicle is responsible for placing the JWST in a Transfer Orbit. This transfer orbit is the trajectory from which the JWST can transfer itself to its operational orbit.\n",
      "\n",
      "Final Answer: Yes.<|end_of_turn|>\n"
     ]
    }
   ],
   "source": [
    "output = \"\"\"<s> GPT4 Correct User:System: |JWST| |isPlacedIn| |Transfer Orbit| by |Launch Vehicle|\n",
    "    |JWST| |orbits| |Second LaGrange Point (L2)|\n",
    "    |JWST| |maintains_ZExcursion| \"≤500,000 Km\"|\n",
    "    |JWST| |maintains_YExcursion| \"≤800,000 Km\"|\n",
    "    |JWST| |avoids| |Eclipse|\n",
    "    |JWST| |transfersTo| |L2 Operational Orbit| using |DUA IRD (JWST-IRD-003674)|\n",
    "    |JWST| |has_MissionLifetime| \"5 years\"|\n",
    "    |JWST| |completes_Commissioning| within \"6 months\"|\n",
    "    |JWST System| |delivers_RealTimeTelemetry| \"92.5%\"|\n",
    "    |JWST System| |archives_RecordedData| \"97%\"|\n",
    "    |JWST| |achieves_Sensitivity| per |NIRCam, NIRSpec, MIRI, FGS-TF Calculations|\n",
    "    \n",
    "Requirement: The Launch Vehicle shall place the Observatory on a trajectory from which the Observatory can transfer itself to its operational orbit.\n",
    "Based on the system description, is this requirement satisfied? Include in your answer \"Final Answer: Yes/No\". Use Python scripting if you need to calculate something by starting with '''python... \n",
    "GPT4 Correct Assistant:Let's analyze the given System and requirement step-by-step:\n",
    "\n",
    "System:\n",
    "- |JWST| |isPlacedIn| |Transfer Orbit| by |Launch Vehicle|\n",
    "\n",
    "Requirement:\n",
    "- The Launch Vehicle shall place the Observatory on a trajectory from which the Observatory can transfer itself to its operational orbit.\n",
    "\n",
    "From the given system information, we can see that the Launch Vehicle is responsible for placing the JWST in a Transfer Orbit. This transfer orbit is the trajectory from which the JWST can transfer itself to its operational orbit.\n",
    "\n",
    "Final Answer: Yes.<|end_of_turn|>\"\"\" \n",
    "\n",
    "def parse_output(output, system_prompt, cot=True):\n",
    "\n",
    "    if cot: \n",
    "        output = output.split(\"Let's analyze the given System and requirement step-by-step:\")[-1]        \n",
    "    else: \n",
    "        output = output.split(system_prompt)[-1]\n",
    "\n",
    "    return output\n",
    "    \n",
    "print(parse_output(output, \"GPT4 Correct Assistant:\", cot= True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from test_reasoning import evaluate, generate_cpu_gpu\n",
    "# from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "# MAX_NEW_TOKENS = 512\n",
    "# extra_args = {\"do_sample\":True, \"max_new_tokens\": MAX_NEW_TOKENS, \"num_return_sequences\" : 1}\n",
    "# temperature=0.8\n",
    "# #top_k = generation_config.top_k, \n",
    "# num_beams= 1\n",
    "# top_p = 0.95\n",
    "# num_return_sequences=1\n",
    "# no_repeat_ngram_size = 8\n",
    "\n",
    "\n",
    "# generation_config = GeneraAtionConfig(\n",
    "#         temperature=temperature,\n",
    "#         top_p=top_p,\n",
    "#         #top_k=top_k,\n",
    "#         num_beams=num_beams,\n",
    "#         no_repeat_ngram_size= no_repeat_ngram_size,\n",
    "#         **extra_args,\n",
    "#      )\n",
    "\n",
    "# prompt_reason = test \n",
    "\n",
    "# output = evaluate(prompt_reason, generation_config, model = model, tokenizer= tokenizer, device = \"cuda\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NEW_TOKEN = 512\n",
    "extra_args = {  \"max_new_tokens\":MAX_NEW_TOKEN,\n",
    "                \"min_new_tokens\": 256, \n",
    "                \"do_sample\":True, \n",
    "                \"num_beams\" : 1, \n",
    "                \"num_return_sequences\" : 1, \n",
    "                #\"no_repeat_ngram_size\": 12, \n",
    "                \"temperature\": 0.8, \n",
    "                \"top_p\": 0.95,\n",
    "                \"begin_suppress_tokens\": [2], \n",
    "                }\n",
    "\n",
    "inputs = tokenizer(prompt_reason, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "generation_config = GenerationConfig(**extra_args)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "    )\n",
    "    s = generation_output.sequences[0]\n",
    "output = tokenizer.decode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_reasoning import evaluate, generate_cpu_gpu\n",
    "\n",
    "from transformers import GenerationConfig\n",
    "import os \n",
    "generation_config = GenerationConfig()\n",
    "\n",
    "prompt_path = os.path.join(\"\", './Machine_Learning/prompt_reason.txt')\n",
    "with open(prompt_path, 'r') as f:\n",
    "        prompt_reason = f.read()\n",
    "\n",
    "req= \"The Observatory shall orbit the Second LaGrange Point (L2) of the Sun–Earth system.\"\n",
    "req = \"The operational JWST System shall maintain the excursion of the orbit about L2 in the Z direction (defined in Figure 3-2) to less than or equal to 500,000 Km.\"\n",
    "\n",
    "definition = \"\"\"|JWST| |isPlacedIn| |Transfer Orbit| by |Launch Vehicle|\n",
    "    |JWST| |orbits| |Second LaGrange Point (L2)|\n",
    "    |JWST| |maintains_ZExcursion| \"≤500,000 Km\"|\n",
    "    |JWST| |maintains_YExcursion| \"≤800,000 Km\"|\n",
    "    |JWST| |avoids| |Eclipse|\n",
    "    |JWST| |transfersTo| |L2 Operational Orbit| using |DUA IRD (JWST-IRD-003674)|\n",
    "    |JWST| |has_MissionLifetime| \"5 years\"|\n",
    "    |JWST| |completes_Commissioning| within \"6 months\"|\n",
    "    |JWST System| |delivers_RealTimeTelemetry| \"92.5%\"|\n",
    "    |JWST System| |archives_RecordedData| \"97%\"|\n",
    "    |JWST| |achieves_Sensitivity| per |NIRCam, NIRSpec, MIRI, FGS-TF Calculations|\n",
    "    \"\"\" \n",
    "\n",
    "MAX_NEW_TOKEN = 512\n",
    "\n",
    "prompt_reason = f\"{prompt_reason}\\n System:{definition} \\nRequirement: {req} \\nASSISTANT: Let's analyze the given System and requirement step-by-step:\"\n",
    "\n",
    "output, complete_output = evaluate(prompt_reason,model = model, tokenizer = tokenizer, device= \"cuda\", temperature=0.8, top_k = generation_config.top_k, num_beams= 1, top_p = 0.95, do_sample=True, max_new_tokens=MAX_NEW_TOKEN, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature=0.8\n",
    "top_k = generation_config.top_k \n",
    "num_beams= 1\n",
    "top_p = 0.95\n",
    "no_repeat_ngram_size = 8\n",
    "device = 'cuda'\n",
    "\n",
    "\n",
    "MAX_NEW_TOKEN = 512\n",
    "\n",
    "keyargs = {\"do_sample\":True, \"num_return_sequences\": 1}\n",
    "max_new_tokens = MAX_NEW_TOKEN\n",
    "\n",
    "inputs = tokenizer(prompt_reason, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    top_k=top_k,\n",
    "    num_beams=num_beams,\n",
    "    no_repeat_ngram_siz= no_repeat_ngram_size,\n",
    "    **keyargs,\n",
    ")\n",
    "\n",
    "#print(temperature)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "    )\n",
    "    s = generation_output.sequences[0]\n",
    "output = tokenizer.decode(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_updated",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
