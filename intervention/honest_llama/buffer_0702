alpha: 5 K: 4
RQ-8-5
RQ-2-5
RQ-11
RQ-12
RQ-2-6
RQ-2-7
RQ-2-12
RQ-2-8
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 31.12it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|                                                                                                        | 0/2 [00:00<?, ?it/s]/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.04s/it]
/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
0it [00:00, ?it/s]2024-02-07 13:47:25.089569: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
47it [15:37, 19.94s/it]
Precision: 0.4166666666666667, Recall: 1.0 for fold 0 and alpha 5.0 and number of heads 4
predict
yes          25
no           20
undefined     2
Name: count, dtype: int64


alpha: 5 K: 16
RQ-8-5
RQ-2-5
RQ-11
RQ-12
RQ-2-6
RQ-2-7
RQ-2-12
RQ-2-8
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 30.53it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|                                                                                                        | 0/2 [00:00<?, ?it/s]/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:19<00:00,  9.78s/it]
/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
0it [00:00, ?it/s]2024-02-07 14:03:31.881833: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
47it [17:23, 22.20s/it]
Precision: 0.4375, Recall: 0.875 for fold 0 and alpha 5.0 and number of heads 16
predict
no           26
yes          18
undefined     3
Name: count, dtype: int64


alpha: 5 K: 32
RQ-8-5
RQ-2-5
RQ-11
RQ-12
RQ-2-6
RQ-2-7
RQ-2-12
RQ-2-8
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:00<00:00, 32.46it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|                                                                                                        | 0/2 [00:00<?, ?it/s]/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:16<00:00,  8.42s/it]
/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
0it [00:00, ?it/s]2024-02-07 14:21:21.741566: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
47it [16:09, 20.64s/it]
Precision: 0.2777777777777778, Recall: 0.8333333333333334 for fold 0 and alpha 5.0 and number of heads 32
predict
no           28
yes          18
undefined     1
Name: count, dtype: int64


alpha: 5 K: 64
RQ-8-5
RQ-2-5
RQ-11
RQ-12
RQ-2-6
RQ-2-7
RQ-2-12
RQ-2-8
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 30.43it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|                                                                                                        | 0/2 [00:00<?, ?it/s]/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:19<00:00,  9.89s/it]
/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
0it [00:00, ?it/s]2024-02-07 14:38:03.120851: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
47it [14:11, 18.11s/it]
Precision: 0.36363636363636365, Recall: 0.5714285714285714 for fold 0 and alpha 5.0 and number of heads 64
predict
no           30
yes          13
undefined     4
Name: count, dtype: int64


alpha: 15 K: 4
RQ-8-5
RQ-2-5
RQ-11
RQ-12
RQ-2-6
RQ-2-7
RQ-2-12
RQ-2-8
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 30.58it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|                                                                                                        | 0/2 [00:00<?, ?it/s]/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:15<00:00,  7.83s/it]
/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
0it [00:00, ?it/s]2024-02-07 14:52:39.360379: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
7it [01:56, 19.40s/it]
47it [14:53, 19.02s/it]
Precision: 0.4166666666666667, Recall: 1.0 for fold 0 and alpha 15.0 and number of heads 4
predict
yes    26
no     21
Name: count, dtype: int64


alpha: 15 K: 16
RQ-8-5
RQ-2-5
RQ-11
RQ-12
RQ-2-6
RQ-2-7
RQ-2-12
RQ-2-8
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 30.74it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|                                                                                                        | 0/2 [00:00<?, ?it/s]/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:16<00:00,  8.42s/it]
/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
0it [00:00, ?it/s]2024-02-07 15:08:00.405401: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
47it [14:35, 18.63s/it]
Precision: 0.6363636363636364, Recall: 0.875 for fold 0 and alpha 15.0 and number of heads 16
predict
no           30
yes          13
undefined     4
Name: count, dtype: int64


alpha: 15 K: 32
RQ-8-5
RQ-2-5
RQ-11
RQ-12
RQ-2-6
RQ-2-7
RQ-2-12
RQ-2-8
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 30.70it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|                                                                                                        | 0/2 [00:00<?, ?it/s]/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:17<00:00,  8.78s/it]
/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
0it [00:00, ?it/s]2024-02-07 15:23:04.728706: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
29it [13:52, 34.30s/it]
47it [22:04, 28.18s/it]
Precision: 0.0, Recall: 0.0 for fold 0 and alpha 15.0 and number of heads 32
predict
no           28
yes          13
undefined     6
Name: count, dtype: int64


alpha: 15 K: 64
RQ-8-5
RQ-2-5
RQ-11
RQ-12
RQ-2-6
RQ-2-7
RQ-2-12
RQ-2-8
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 28.83it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|                                                                                                        | 0/2 [00:00<?, ?it/s]/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:16<00:00,  8.03s/it]
/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
0it [00:00, ?it/s]2024-02-07 15:45:35.774600: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
47it [16:26, 21.00s/it]
Precision: 0, Recall: 0 for fold 0 and alpha 15.0 and number of heads 64
predict
no           44
undefined     3
Name: count, dtype: int64


alpha: 20 K: 4
RQ-8-5
RQ-2-5
RQ-11
RQ-12
RQ-2-6
RQ-2-7
RQ-2-12
RQ-2-8
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 30.75it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|                                                                                                        | 0/2 [00:00<?, ?it/s]/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:15<00:00,  7.84s/it]
/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
0it [00:00, ?it/s]2024-02-07 16:02:28.171366: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
47it [14:55, 19.04s/it]
Precision: 0.28125, Recall: 0.6428571428571429 for fold 0 and alpha 20.0 and number of heads 4
predict
yes          32
no           14
undefined     1
Name: count, dtype: int64


alpha: 20 K: 16
RQ-8-5
RQ-2-5
RQ-11
RQ-12
RQ-2-6
RQ-2-7
RQ-2-12
RQ-2-8
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 30.66it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|                                                                                                        | 0/2 [00:00<?, ?it/s]/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:15<00:00,  7.93s/it]
/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
0it [00:00, ?it/s]2024-02-07 16:17:49.211036: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
34it [11:35, 20.09s/it]
47it [16:20, 20.86s/it]
Precision: 0.625, Recall: 0.8333333333333334 for fold 0 and alpha 20.0 and number of heads 16
predict
no           32
yes           9
undefined     6
Name: count, dtype: int64


alpha: 20 K: 32
RQ-8-5
RQ-2-5
RQ-11
RQ-12
RQ-2-6
RQ-2-7
RQ-2-12
RQ-2-8
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 30.90it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|                                                                                                        | 0/2 [00:00<?, ?it/s]/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:18<00:00,  9.47s/it]
/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
0it [00:00, ?it/s]2024-02-07 16:34:38.742421: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
22it [15:01, 48.28s/it]
47it [32:26, 41.42s/it]
Precision: 0.0, Recall: 0.0 for fold 0 and alpha 20.0 and number of heads 32
predict
no           24
undefined    18
yes           5
Name: count, dtype: int64


alpha: 20 K: 64
RQ-8-5
RQ-2-5
RQ-11
RQ-12
RQ-2-6
RQ-2-7
RQ-2-12
RQ-2-8
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 30.42it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|                                                                                                        | 0/2 [00:00<?, ?it/s]/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:15<00:00,  7.67s/it]
/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
0it [00:00, ?it/s]2024-02-07 17:07:31.329678: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
46it [30:59, 35.60s/it]
47it [31:59, 40.85s/it]
Precision: 0, Recall: 0 for fold 0 and alpha 20.0 and number of heads 64
predict
no           28
undefined    19
Name: count, dtype: int64


alpha: 35 K: 4
RQ-8-5
RQ-2-5
RQ-11
RQ-12
RQ-2-6
RQ-2-7
RQ-2-12
RQ-2-8
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 31.19it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|                                                                                                        | 0/2 [00:00<?, ?it/s]/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:17<00:00,  8.91s/it]
/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
0it [00:00, ?it/s]2024-02-07 17:39:59.382056: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
25it [09:21, 25.49s/it]

47it [17:44, 22.65s/it]
Precision: 0.16666666666666666, Recall: 0.6666666666666666 for fold 0 and alpha 35.0 and number of heads 4
predict
yes          38
no            8
undefined     1
Name: count, dtype: int64


alpha: 35 K: 16
RQ-8-5
RQ-2-5
RQ-11
RQ-12
RQ-2-6
RQ-2-7
RQ-2-12
RQ-2-8
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 30.70it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|                                                                                                        | 0/2 [00:00<?, ?it/s]/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:15<00:00,  7.95s/it]
/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
0it [00:00, ?it/s]2024-02-07 17:58:09.415993: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
47it [21:03, 26.88s/it]
Precision: 0.08333333333333333, Recall: 0.1111111111111111 for fold 0 and alpha 35.0 and number of heads 16
predict
no           32
yes          12
undefined     3
Name: count, dtype: int64


alpha: 35 K: 32
RQ-8-5
RQ-2-5
RQ-11
RQ-12
RQ-2-6
RQ-2-7
RQ-2-12
RQ-2-8
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 31.10it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|                                                                                                        | 0/2 [00:00<?, ?it/s]/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.46s/it]
/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
0it [00:00, ?it/s]2024-02-07 18:19:30.355103: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
47it [45:23, 57.94s/it]
Precision: 0, Recall: 0 for fold 0 and alpha 35.0 and number of heads 32
predict
undefined    47
Name: count, dtype: int64


alpha: 35 K: 64
RQ-8-5
RQ-2-5
RQ-11
RQ-12
RQ-2-6
RQ-2-7
RQ-2-12
RQ-2-8
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 30.70it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|                                                                                                        | 0/2 [00:00<?, ?it/s]/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:06<00:00,  3.38s/it]
/home/paul/miniconda3/envs/iti/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
0it [00:00, ?it/s]2024-02-07 19:05:09.646827: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
47it [46:09, 58.92s/it]
Precision: 0, Recall: 0 for fold 0 and alpha 35.0 and number of heads 64
predict
undefined    47
Name: count, dtype: int64

